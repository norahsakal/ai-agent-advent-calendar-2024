{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0c77d554-ddd8-447e-804b-8b0bf20c2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from IPython.display import display, Image, HTML\n",
    "\n",
    "# LlamaIndex\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# LlamaIndex agents\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, AgentRunner\n",
    "\n",
    "# LlamaIndex LLMs\n",
    "from llama_index.llms.openai import OpenAI as OpenAI_Llama\n",
    "\n",
    "# LlamaIndex metadata filters\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilters,FilterCondition\n",
    ")\n",
    "\n",
    "# LlamaIndex retrievers\n",
    "from llama_index.core.retrievers import VectorIndexAutoRetriever, VectorIndexRetriever\n",
    "\n",
    "# LlamaIndex vector stores\n",
    "from llama_index.core.vector_stores import MetadataInfo, VectorStoreInfo\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "\n",
    "# Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5994f-4da4-40af-9511-16fa120a3a1d",
   "metadata": {},
   "source": [
    "# Advent Calendar Day 1: How AI Agents Improve Naive Chatbots by Asking Clarifying Questions\n",
    "\n",
    "This December, we're highlighting the limitations of simple AI chatbots in online retail and demonstrating how **AI agents** enhance customer interactions.\n",
    "\n",
    "Each day, we'll explore a common challenge faced by naive Retrieval Augmented Generation (RAG) chatbot systems and show how AI agents overcome them. \n",
    "\n",
    "Todays topic is about how AI agents improve naive chatbots by asking clarifying questions.\n",
    "\n",
    "![Cover image](images/1_dec/1_dec_cover.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68ece46-df06-480c-a3f0-2a3e6c8df93b",
   "metadata": {},
   "source": [
    "## Introducing SoleMates\n",
    "\n",
    "***SoleMates*** is a fictional online shoe store that we'll use as a practical example throughout this tutorial.\n",
    "\n",
    "We'll explore interactions between customers and chatbots at SoleMates, highlighting the differences between basic chatbots and advanced AI agents.\n",
    "\n",
    "![SoleMates Illustration](images/solemates.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ee172-c3a7-4ca5-a1a8-49589b4faa1e",
   "metadata": {},
   "source": [
    "## Today's Challenge: No Reflection - Simple Chatbots Can't Infer from Context\n",
    "\n",
    "### Scenario\n",
    "\n",
    "A customer initiates a chat with **SoleMates**:\n",
    "\n",
    "**Customer:** \"I need shoes for a black-tie event\"\n",
    "\n",
    "![A customer initiates a chat with SoleMates](images/1_dec/3_customer_black_tie.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b450882-00c9-4262-a421-b9be0dd81638",
   "metadata": {},
   "source": [
    "## Load Shoe Data\n",
    "\n",
    "Let's start by reading the SoleMates shoe dataset. This dataset contains detailed product information, such as shoe colors and heel heights, which we'll transform into embeddings and store in a cloud-based Pinecone vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b37126db-d68a-4a3c-94f1-9a04471894c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>color</th>\n",
       "      <th>color_details</th>\n",
       "      <th>usage</th>\n",
       "      <th>product_title</th>\n",
       "      <th>image</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>heels_height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>sports shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[neon green]</td>\n",
       "      <td>sports</td>\n",
       "      <td>Adidas men eqt nitro fashion black sports shoes</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>120</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>sports shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[white]</td>\n",
       "      <td>sports</td>\n",
       "      <td>Puma men's yugorun black white shoe</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>boots</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>casual</td>\n",
       "      <td>Timberland men black casual shoes</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>casual shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>casual</td>\n",
       "      <td>Provogue men black shoes</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>125</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>formal shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>formal</td>\n",
       "      <td>Lee cooper men black shoe</td>\n",
       "      <td>5.jpg</td>\n",
       "      <td>155</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id gender  category sub_category  product_type  color  \\\n",
       "0           1    men  footwear        shoes  sports shoes  black   \n",
       "1           2    men  footwear        shoes  sports shoes  black   \n",
       "2           3    men  footwear        shoes         boots  black   \n",
       "3           4    men  footwear        shoes  casual shoes  black   \n",
       "4           5    men  footwear        shoes  formal shoes  black   \n",
       "\n",
       "  color_details   usage                                    product_title  \\\n",
       "0  [neon green]  sports  Adidas men eqt nitro fashion black sports shoes   \n",
       "1       [white]  sports              Puma men's yugorun black white shoe   \n",
       "2            []  casual                Timberland men black casual shoes   \n",
       "3            []  casual                         Provogue men black shoes   \n",
       "4            []  formal                        Lee cooper men black shoe   \n",
       "\n",
       "   image  price_usd  heels_height  \n",
       "0  1.jpg        120          <NA>  \n",
       "1  2.jpg         50          <NA>  \n",
       "2  3.jpg         60          <NA>  \n",
       "3  4.jpg        125          <NA>  \n",
       "4  5.jpg        155          <NA>  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the SoleMates shoe dataset\n",
    "df_shoes = pd.read_csv('data/solemates_shoe_directory.csv')\n",
    "\n",
    "# Convert 'color_details' from string representation of a list to an actual list\n",
    "df_shoes['color_details'] = df_shoes['color_details'].apply(ast.literal_eval)\n",
    "\n",
    "# Ensure 'heels_height' is treated as a nullable integer type\n",
    "df_shoes['heels_height'] = df_shoes['heels_height'].astype('Int64')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df_shoes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b45b36-59b2-4aff-bafe-c95eb16c731f",
   "metadata": {},
   "source": [
    "## Cost of Vectorization and Pre-Embedded Dataset\n",
    "\n",
    "Vectorizing datasets with AWS Bedrock and the Titan multimodal model involves costs based on the number of input tokens and images:\n",
    "\n",
    "- **Text embeddings**: $0.0008 per 1,000 input tokens  \n",
    "\n",
    "- **Image embeddings**: $0.00006 per image  \n",
    "\n",
    "The provided SoleMates dataset is small, containing just 85 pairs of shoes, making it affordable to vectorize. For this dataset, I calculated the total cost of vectorization and summarized the token counts below:\n",
    "\n",
    "- **Token Count**: `831` tokens  \n",
    "- **Total Cost**: `$0.0058`  \n",
    "\n",
    "If you prefer not to generate embeddings yourself or don't have access to AWS, you can use a pre-embedded dataset that I've prepared as a CSV file. This file includes all embeddings and token counts, allowing you to follow the guide without incurring additional costs. However, for hands-on experience, I recommend running the embedding process to understand the workflow.\n",
    "\n",
    "To load the pre-embedded dataset, use the following code:\n",
    "```python\n",
    "# Load pre-embedded dataset\n",
    "df_shoes = pd.read_csv('data/solemates_shoe_directory_pre_embedded_shoes.csv')\n",
    "```\n",
    "This step is entirely optional and designed to accommodate various levels of access and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53134c90-80fc-4ba2-a75f-041dda15a60e",
   "metadata": {},
   "source": [
    "### Prepare Amazon Bedrock for Embedding Generation\n",
    "\n",
    "To vectorize our product data, we'll generate embeddings for each product using AWS Titan. These embeddings combine image and text data to represent each product in a format suitable for search and recommendation systems.\n",
    "\n",
    ">**Important Note on Cost**:  \n",
    ">Vectorizing datasets incurs a cost. The SoleMates dataset contains 85 pairs of shoes, resulting in an estimated total cost of `$0.0058`.\n",
    ">\n",
    ">I've added a token count column to help track these costs, and you can calculate your own total for larger datasets.\n",
    "\n",
    "If you'd rather not generate embeddings yourself, you can load a pre-embedded version of the dataset I've provided. This is entirely optional but ensures you can still follow along with the guide:\n",
    "```python\n",
    "# Load pre-embedded dataset\n",
    "df_shoes = pd.read_csv('data/solemates_shoe_directory_pre_embedded_shoes.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5937c5f-52a7-454d-af40-d2930888164c",
   "metadata": {},
   "source": [
    "## Getting started with Amazon Bedrock\n",
    "To use Amazon Bedrock for embedding generation, start by setting up your AWS environment:\n",
    "\n",
    "1. Create an AWS account if you don't already have one\n",
    "2. Set up an AWS Identity and Access Management (IAM) role with permissions tailored for Amazon Bedrock\n",
    "3. Submit a request to access the foundation models (FMs) you'd like to use\n",
    "\n",
    "Next, we'll initialize the Bedrock runtime client, which allows us to interact with AWS Titan for embedding generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773bd1f3-74f1-43a5-8c72-1a008ef5f96e",
   "metadata": {},
   "source": [
    "## Set up AWS Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1c643a-0e10-49f8-bc89-0f5284c376de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your AWS profile \n",
    "# Replace 'your-profile-name' with the name of your AWS CLI profile\n",
    "# To use your default AWS profile, leave 'aws_profile' as None\n",
    "aws_profile = os.environ.get('AWS_PROFILE')\n",
    "\n",
    "# Specify the AWS region where Bedrock is available\n",
    "aws_region_name = \"us-east-1\"\n",
    "\n",
    "try:\n",
    "    # Set the default session for the specified profile\n",
    "    if aws_profile:\n",
    "        boto3.setup_default_session(profile_name=aws_profile)\n",
    "    else:\n",
    "        boto3.setup_default_session()  # Use default AWS profile if none is specified\n",
    "    \n",
    "    # Initialize the Bedrock runtime client\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=aws_region_name\n",
    "    )\n",
    "except NoCredentialsError:\n",
    "    print(\"AWS credentials not found. Please configure your AWS profile.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469208e5-3388-4f70-8c2a-c0399a6904a3",
   "metadata": {},
   "source": [
    "## Generate Embeddings for Product Data\n",
    "\n",
    "To prepare our product data for the vector database, we'll generate embeddings for each product using AWS Titan. These embeddings combine image and text data to represent each product in a format suitable for search and recommendation systems.\n",
    "\n",
    "Before generating embeddings, we'll initialize two new columns in the dataset:\n",
    "- **`titan_embedding`**: To store the embedding vectors.\n",
    "- **`token_count`**: To store the token count for each product title.\n",
    "\n",
    "Then, we'll define a function to generate embeddings and apply it to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003c254-97ca-486d-bf2b-859f0a0a8412",
   "metadata": {},
   "source": [
    "## Initialize Columns for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74eef96-fef3-4bfe-ac6c-0788e8dd3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize columns to store embeddings and token counts\n",
    "df_shoes['titan_embedding'] = None  # Placeholder for embedding vectors\n",
    "df_shoes['token_count'] = None  # Placeholder for token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b450e6af-4cdb-4607-927f-d634b4250674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to generate image and text embeddings\n",
    "def generate_embeddings(df, image_col='image', text_col='product_title', embedding_col='embedding', image_folder='data/footwear'):\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Generating embeddings\"):\n",
    "        try:\n",
    "            # Prepare image file as base64\n",
    "            image_path = os.path.join(image_folder, row[image_col])\n",
    "            with open(image_path, 'rb') as img_file:\n",
    "                image_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "            \n",
    "            # Create input data for the model\n",
    "            input_data = {\"inputImage\": image_base64, \"inputText\": row[text_col]}\n",
    "\n",
    "            # Invoke AWS Titan model via Bedrock runtime\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=json.dumps(input_data),\n",
    "                modelId=\"amazon.titan-embed-image-v1\",\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "            # Extract embedding and token count from response\n",
    "            embedding = response_body.get(\"embedding\")\n",
    "            token_count = response_body.get(\"inputTextTokenCount\")\n",
    "\n",
    "            # Validate and save the embedding\n",
    "            if isinstance(embedding, list):\n",
    "                df.at[index, embedding_col] = embedding  # Save embedding as a list\n",
    "                df.at[index, 'token_count'] = int(token_count)  # Save token count as an integer\n",
    "            else:\n",
    "                raise ValueError(\"Embedding is not a list as expected.\")\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Error for row {index}: {e}\")\n",
    "            df.at[index, embedding_col] = None  # Handle errors gracefully\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca417c9-8156-4653-87da-d54008e350e4",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51db4597-71ba-4aa3-813d-56d97669579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "enerating embeddings: 100%|████████████████████████████████████████████████████████| 85/85 [00:24<00:00,  3.47it/s]"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for the product data\n",
    "df_shoes = generate_embeddings(df=df_shoes, embedding_col='titan_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b828c6-886f-481f-9f15-3093f9f2ef76",
   "metadata": {},
   "source": [
    "## Save Dataset for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "691ec066-c5e1-4364-8acd-65d84ce220ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with embeddings saved as 'shoes_with_embeddings_token_2024_12_06.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the dataset with generated embeddings to a CSV file\n",
    "# Get today's date in YYYY_MM_DD format\n",
    "today = datetime.now().strftime('%Y_%m_%d')\n",
    "\n",
    "# Save the dataset with generated embeddings to a CSV file\n",
    "df_shoes.to_csv(f'shoes_with_embeddings_token_{today}.csv', index=False)\n",
    "print(f\"Dataset with embeddings saved as 'shoes_with_embeddings_token_{today}.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f0538-ee08-4f42-8e6c-693bc480da80",
   "metadata": {},
   "source": [
    "## Create a Dictionary with Product Data\n",
    "\n",
    "Before we create LlamaIndex `Document` objects, we need to structure the product data into dictionaries. These dictionaries include:\n",
    "\n",
    "1. **Text**: The product title that will be used for embedding queries.\n",
    "2. **Metadata**: A dictionary containing detailed attributes for each product (e.g., color, gender, usage, price).\n",
    "3. **Embedding**: The Titan embeddings generated earlier.\n",
    "\n",
    "This dictionary format ensures the data is well-organized for creating `Document` objects in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f2408c-86fe-437c-96e8-ee42718bfd63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame rows into a list of dictionaries for LlamaIndex\n",
    "product_data = df_shoes.apply(lambda row: {\n",
    "    'text': row['product_title'],\n",
    "    'metadata': {\n",
    "        'color': row['color'],\n",
    "        'text': row['product_title'],\n",
    "        'gender': row['gender'],\n",
    "        'product_type': row['product_type'],\n",
    "        'usage': row['usage'],\n",
    "        'price': row['price_usd'],\n",
    "        'product_id': row['product_id'],\n",
    "        **({'heels_height': int(row['heels_height'])} if not pd.isna(row['heels_height']) else {}),\n",
    "        **({'color_details': row['color_details']} if row['color_details'] else {})\n",
    "    },\n",
    "    'embedding': row['titan_embedding']\n",
    "}, axis=1).tolist()\n",
    "\n",
    "# Preview the first product dictionary\n",
    "#product_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7e75d-38c0-4a21-a064-ad24ffc13a50",
   "metadata": {},
   "source": [
    "## Create LlamaIndex Documents\n",
    "\n",
    "We'll now use the product data dictionaries to create LlamaIndex `Document` objects. \n",
    "\n",
    "These `Documents` are crucial because:\n",
    "\n",
    "- They act as containers for our product data and embeddings.\n",
    "- They enable seamless interaction with Pinecone for upserting embeddings.\n",
    "\n",
    "Each `Document` includes:\n",
    "1. The **text** (product title) for embedding and query purposes\n",
    "2. **Metadata** with attributes like color, gender, and price\n",
    "3. The **embedding** generated earlier\n",
    "4. An **exclusion list** (`excluded_embed_metadata_keys`) to prevent unnecessary metadata fields from being embedded, ensuring optimal performance and cost-efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd4dbb-046b-4ccc-81ae-76f73c7ab5a6",
   "metadata": {},
   "source": [
    "## Create LlamaIndex Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "150978c8-c65f-4bfc-a6e7-b96972e35b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'color': 'black',\n",
       " 'text': 'Adidas men eqt nitro fashion black sports shoes',\n",
       " 'gender': 'men',\n",
       " 'product_type': 'sports shoes',\n",
       " 'usage': 'sports',\n",
       " 'price': 120,\n",
       " 'product_id': 1,\n",
       " 'color_details': ['neon green']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create LlamaIndex Document objects\n",
    "documents = []\n",
    "for doc in product_data:\n",
    "    documents.append(\n",
    "        Document(\n",
    "            text=doc[\"text\"],\n",
    "            extra_info=doc[\"metadata\"],\n",
    "            embedding=doc['embedding'],\n",
    "            \n",
    "            # Avoid embedding unnecessary metadata\n",
    "            excluded_embed_metadata_keys=[\n",
    "                'color',\n",
    "                'gender',\n",
    "                'product_type',\n",
    "                'usage',\n",
    "                'text',\n",
    "                'price',\n",
    "                'product_id',\n",
    "                'heels_height',\n",
    "                'color_details'\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Confirm the first Document object\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e952186-1753-4d4d-a3a2-70ba897ac888",
   "metadata": {},
   "source": [
    "## Initialize Pinecone\n",
    "\n",
    "To interact with Pinecone, you'll first need an account and API keys. If you don't already have them, [create a Pinecone account](https://www.pinecone.io/) and retrieve your API key.\n",
    "\n",
    "Pinecone is a vector database designed to store and query embeddings. We'll use Pinecone to upsert the AWS Titan embeddings we generated earlier, enabling efficient similarity and hybrid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9aad5db-4754-45bb-86dd-6f2a6d4206bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone client with API key\n",
    "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "index_name = \"solemates\"  # Replace with your desired index name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41932e99-12f7-420e-ba70-de784970fa7b",
   "metadata": {},
   "source": [
    "## List Current Indexes\n",
    "\n",
    "Let's list the existing indexes in your Pinecone account to ensure no duplicates before creating a new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68064603-74ca-4950-bcef-74c03cd97b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': []}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List current indexes\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fff0f-c4fb-4c0f-8773-4be229abf3dd",
   "metadata": {},
   "source": [
    "## Create Index\n",
    "\n",
    "Next, we'll create a Pinecone index. An index stores the embeddings and metadata for your data.\n",
    "\n",
    "- **Dimension**: Matches the size of the embeddings we're using (1024 for AWS Titan multimodal embeddings)\n",
    "- **Metric**: Defines how similarity is calculated (e.g., dot product, cosine similarity)\n",
    "- **ServerlessSpec**: Specifies the cloud provider and region for your index\n",
    "\n",
    "If the index already exists, this step will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ae7c9a7-3d63-4804-b667-6ef15e014347",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,  # AWS Titan embeddings require 1024 dimensions\n",
    "        metric=\"dotproduct\",  # Required for hybrid search with Pinecone\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f37e20-afe2-4695-a5db-2e5202ea0f7f",
   "metadata": {},
   "source": [
    "## Inspect Pinecone Index\n",
    "\n",
    "Navigate to your Pinecone dashboard, and you should now see your new index with **0 records (vectors)**, as it hasn't been populated yet:\n",
    "\n",
    "![Pinecone shows an empty index](images/pinecone/4_pinecone_empty_index.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc931ec8-278e-4be1-bb5b-cc1cd55d072f",
   "metadata": {},
   "source": [
    "## Initialize Pinecone Index\n",
    "\n",
    "After creating the index, we'll initialize it for further operations like upserting embeddings and querying vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d63f315-49f2-402b-8b19-424e60d23438",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a34df-bd42-4a16-a850-faa153291855",
   "metadata": {},
   "source": [
    "## Create Pinecone Vector Store\n",
    "\n",
    "We'll now set up a **Pinecone Vector Store** using LlamaIndex. \n",
    "\n",
    "This vector store connects our Pinecone index with the LlamaIndex framework.\n",
    "\n",
    "Key configuration details:\n",
    "1. **Namespace**: A logical grouping within the index, allowing future addition of other product types.\n",
    "2. **Hybrid Search**: Enabling both semantic and keyword search by adding sparse vectors.\n",
    "\n",
    "For more information:\n",
    "- [Pinecone Namespaces Guide](https://docs.pinecone.io/guides/indexes/use-namespaces)\n",
    "- [Hybrid Search Introduction](https://www.pinecone.io/learn/hybrid-search-intro/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1332ed46-d9e5-4048-8817-b7242b69cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "vector_store = PineconeVectorStore(\n",
    "    pinecone_index=pinecone_index,\n",
    "    namespace='footwear',  # Logical namespace for shoe data\n",
    "    add_sparse_vector=True  # Enables hybrid search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91796b22-3028-4e63-98bc-83384506fb2f",
   "metadata": {},
   "source": [
    "## Create an Ingestion Pipeline\n",
    "\n",
    "We'll create an **Ingestion Pipeline** to upsert our vectors into the Pinecone index. \n",
    "No transformations are required since we've pre-generated embeddings with AWS Titan.\n",
    "\n",
    ">**Note**: As of Dec 4 2024, LlamaIndex doesn't abstract AWS Titan multimodal embeddings, so we're using our own vectors directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b59b4745-0392-4426-9db7-f96ef51133c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[],  # No transformations since embeddings are pre-generated\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b133b0-e886-48fd-984b-6851f8e22f27",
   "metadata": {},
   "source": [
    "## Run the Ingestion Pipeline\n",
    "\n",
    "This step upserts the embeddings into Pinecone for storage and querying.\n",
    "\n",
    "- **Cost Note**: Pinecone charges $2.00 per 1M vectors unless you're on the free plan\n",
    "- **Time Note**: It may take a minute or two for the vectors to become visible in your Pinecone index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6475df3-a622-4b7a-9a6b-e4668b32b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline to upsert embeddings into Pinecone\n",
    "pipeline.run(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace33ed-ff9b-451a-8735-c05f3e6e0c23",
   "metadata": {},
   "source": [
    "## Inspect Pinecone Index\n",
    "\n",
    "Now that we've upserted the vectors, navigate back to Pinecone. You should see **85 records** in your index, corresponding to the embeddings we added:\n",
    "\n",
    "![Populated Pinecone index](images/pinecone/5_pinecone_populated_index.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3916b2-9897-47dd-8946-e824b962ea5b",
   "metadata": {},
   "source": [
    "## Test Querying the Vector Store\n",
    "\n",
    "Now that we have upserted all our shoe vectors, let's test querying the vector database.  \n",
    "\n",
    "We'll start by creating a **Vector Store Index** with LlamaIndex. This index will allow us to query the Pinecone index using the same vector store we initialized earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "909e3af8-b9bd-47aa-9b0f-c0837bb95323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vector Store Index\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828376ed-f6c6-4f86-8216-ee7f28396ade",
   "metadata": {},
   "source": [
    "## Query the Vector Database Directly (Without Query Engine or Chat Engine)\n",
    "\n",
    "Before we use a **Query Engine** or **Chat Engine** to interact with the vector database, we'll start with a direct query using a simple retriever.  \n",
    "\n",
    "This approach demonstrates how you can fetch relevant records from the database without involving advanced reasoning, natural language understanding, or conversation tracking. It's a fundamental way to confirm that the embeddings and metadata are stored correctly and the vector database is functioning as expected.\n",
    "\n",
    "Next, we'll move on to more advanced querying techniques, including using a **Query Engine** and an **Agent** to leverage the power of LLMs.\n",
    "\n",
    "The first step is creating a simple retriever, but first, we need to define a custom embedding function. \n",
    "\n",
    "As of Dec 4, 2024, **LlamaIndex does not abstract AWS Titan multimodal embeddings**, so we'll implement a custom class for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327db3ff-fa36-4f98-b93c-4f805cfc9f25",
   "metadata": {},
   "source": [
    "## Create a Function to Request AWS Titan Embeddings\n",
    "\n",
    "We'll define a helper function to request embeddings from AWS Titan's multimodal model. This function will handle both text and image inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5be13cb5-c6e8-4cb0-a547-e5cb9e6b9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_embedding(image_base64=None, text_description=None):\n",
    "    \"\"\"\n",
    "    Request embeddings from AWS Titan multimodal model.\n",
    "\n",
    "    Parameters:\n",
    "        image_base64 (str, optional): Base64 encoded image string.\n",
    "        text_description (str, optional): Text description.\n",
    "\n",
    "    Returns:\n",
    "        list: Embedding vector.\n",
    "    \"\"\"\n",
    "    input_data = {\"inputImage\": image_base64, \"inputText\": text_description}\n",
    "    body = json.dumps(input_data)\n",
    "\n",
    "    # Invoke the Titan multimodal model\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=\"amazon.titan-embed-image-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    if response_body.get(\"message\"):\n",
    "        raise ValueError(f\"Embeddings generation error: {response_body.get('message')}\")\n",
    "\n",
    "    return response_body.get(\"embedding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124454d7-a2bb-4950-9869-07faca07ecaa",
   "metadata": {},
   "source": [
    "## Create Custom Embeddings Class\n",
    "\n",
    "We'll now define a custom embedding class that uses the AWS Titan multimodal model to fetch embeddings. \n",
    "\n",
    "This class overrides key methods in LlamaIndex's `BaseEmbedding` to integrate AWS Titan into the framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a699d370-4f5b-4b66-9a62-6e50bcfb98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEmbeddings(BaseEmbedding):\n",
    "    \"\"\"\n",
    "    Custom embedding class for AWS Titan multimodal embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"multimodal\"\n",
    "    \n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_text_embedding(text)\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get embeddings for a query string.\n",
    "        \"\"\"\n",
    "        return request_embedding(text_description=query)\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get embeddings for a text string.\n",
    "        \"\"\"\n",
    "        return request_embedding(text_description=text)\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Get embeddings for a batch of text strings.\n",
    "        \"\"\"\n",
    "        return [request_embedding(text_description=text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83770b48-66f8-4ce5-892c-8b4e5248873f",
   "metadata": {},
   "source": [
    "## Instantiate the Custom Class\n",
    "\n",
    "We'll now instantiate the `MultimodalEmbeddings` class to use it in our retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "460b5367-53f9-4e50-8bb2-d26650866524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the custom embedding model\n",
    "embed_model = MultimodalEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0bb89-87b5-474b-9ee6-de98632ad561",
   "metadata": {},
   "source": [
    "## Create a Simple Retriever\n",
    "\n",
    "We'll create a simple retriever using the custom embedding model and the vector index.  \n",
    "\n",
    "Key configurations:\n",
    "1. **`similarity_top_k`**: Number of top results to retrieve\n",
    "2. **`vector_store_query_mode`**: Set to **\"hybrid\"** for combining semantic and keyword search\n",
    "3. **`alpha`**: Weighting between semantic (embedding) and keyword search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d7d53-5197-4a19-b99b-4efa1295790d",
   "metadata": {},
   "source": [
    "## Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4bf9953e-2c52-4443-a80b-eda7a6647334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    embed_model=embed_model,\n",
    "    similarity_top_k=8,  # Retrieve the top 8 results\n",
    "    vector_store_query_mode=\"hybrid\",  # Enable hybrid search\n",
    "    alpha=0.5  # Weighting between semantic and keyword search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3c96b-14d8-42aa-a1e3-15efcfd2158f",
   "metadata": {},
   "source": [
    "## Query Vector Database Directly Using a Simple Retriever\n",
    "\n",
    "We'll use a simple retriever to query the vector database and inspect the results. This method interacts with the embeddings and metadata in a straightforward way, without utilizing an LLM-powered **Query Engine** or **Chat Engine**.\n",
    "\n",
    "**Why this step matters**:\n",
    "1. Validates that the vector database is populated correctly\n",
    "2. Shows how to query embeddings directly, bypassing the overhead of LLM-based reasoning\n",
    "3. Prepares the groundwork for building advanced workflows with Query Engines and Agents\n",
    "\n",
    "In the next steps, we'll extend this retriever to integrate with an LLM-powered Query Engine for richer responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d11e8-31a9-45ba-9f5c-84af7973df42",
   "metadata": {},
   "source": [
    "## Query the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ff2ff60-aae2-4f5c-a5a2-344323deb67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2.2395\n",
      "Text: Catwalk women red shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.2250\n",
      "Text: Fila men leonard red shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.2233\n",
      "Text: Basics men red casual shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.2148\n",
      "Text: Carlton london women casual red casual shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.2105\n",
      "Text: Nike men jordan fly wade red sports shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.1867\n",
      "Text: Red tape men brown shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.1822\n",
      "Text: Adidas men blue & red f10 sports shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.1681\n",
      "Text: Red tape men casual brown casual shoes\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Query the vector store for \"red shoes\"\n",
    "results = retriever.retrieve(\"red shoes\")\n",
    "\n",
    "# Display results\n",
    "for item in results:\n",
    "    score = item.score\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(f\"Text: {item.get_content()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5910c5-6a33-41f7-9cdc-5444c1e7cdee",
   "metadata": {},
   "source": [
    "## Visualize Vector Database Pull\n",
    "\n",
    "The vector database query returns a list of red shoes based on the embeddings. To verify the results, let's visualize the pulled vectors.  \n",
    "\n",
    "We'll create a function that loops through the retrieved nodes and displays each image along with its metadata in a row for easy inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d47d7d42-8857-42ac-baed-f08b8a7c7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nodes_with_images_in_row(vector_database_response_nodes, image_folder=\"data/footwear\", img_width=150):\n",
    "    html_content = \"<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\"\n",
    "    \n",
    "    for node in vector_database_response_nodes:\n",
    "        # Retrieve text and product_id from node metadata\n",
    "        text = node.metadata.get('text')\n",
    "        product_id = node.metadata.get('product_id')\n",
    "        \n",
    "        # Generate image path based on product_id\n",
    "        image_path = os.path.join(image_folder, f\"{product_id}.jpg\")\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            # Add each text and image in a flex container\n",
    "            html_content += f\"\"\"\n",
    "                <div style=\"text-align: center;\">\n",
    "                    <p>{text}</p>\n",
    "                    <img src='{image_path}' width='{img_width}px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # Handle missing images gracefully\n",
    "            html_content += f\"\"\"\n",
    "                <div style=\"text-align: center;\">\n",
    "                    <p>{text}</p>\n",
    "                    <p style='color: red;'>Image not found for product_id {product_id}</p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "\n",
    "    # Close the main div\n",
    "    html_content += \"</div>\"\n",
    "    \n",
    "    # Display the content as HTML\n",
    "    display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcb914-4064-4164-8c62-0a5506248c22",
   "metadata": {},
   "source": [
    "## Visualize Shoes\n",
    "Let's visualize the shoes retrieved from the vector database to confirm that the results match the query for \"red shoes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "450e5cde-9e95-48d8-a121-5c9462c0837b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Catwalk women red shoes</p>\n",
       "                    <img src='data/footwear/54.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Fila men leonard red shoes</p>\n",
       "                    <img src='data/footwear/48.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Basics men red casual shoes</p>\n",
       "                    <img src='data/footwear/47.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Carlton london women casual red casual shoes</p>\n",
       "                    <img src='data/footwear/53.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men jordan fly wade red sports shoes</p>\n",
       "                    <img src='data/footwear/45.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Red tape men brown shoes</p>\n",
       "                    <img src='data/footwear/71.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Adidas men blue & red f10 sports shoes</p>\n",
       "                    <img src='data/footwear/36.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Red tape men casual brown casual shoes</p>\n",
       "                    <img src='data/footwear/76.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_nodes_with_images_in_row(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98472c-0429-4b65-8d4c-d3d4641b1ab3",
   "metadata": {},
   "source": [
    "## Examine the Shoes\n",
    "\n",
    "As shown, all the retrieved shoes are red or have red details, confirming that the vector index query works well for focused queries.  \n",
    "\n",
    "Next, we'll involve an LLM to add more flexibility to our queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78d0ef-f8f8-4a3d-b03d-6d1e53af856a",
   "metadata": {},
   "source": [
    "## Why Not Keep Using the Index Alone?\n",
    "\n",
    "The results look great, so why involve an LLM?\n",
    "\n",
    "Let's try querying the vector database with something unrelated, like \"Thank you!\" and examine the response.\n",
    "\n",
    "![Why involve an LLM?](images/1_dec/6_naive_rag_reply.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c34129-ca74-4b6b-b0a8-52be521f20b7",
   "metadata": {},
   "source": [
    "## Test Naive Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a17f636-0d3c-4953-b2f2-2f24b8b95203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.1846\n",
      "Text: Timberland women femmes brown boot\n",
      "--------------------------------------------------\n",
      "Score: 1.1784\n",
      "Text: Nike men's air max black shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1670\n",
      "Text: Nike men's egoli white black shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1652\n",
      "Text: Nike women ten blue white shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1650\n",
      "Text: Nike women zoo blue shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1609\n",
      "Text: Adidas women's piona white shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1591\n",
      "Text: Nike women main draw white blue shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1566\n",
      "Text: Hm women brown shoes\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Timberland women femmes brown boot</p>\n",
       "                    <img src='data/footwear/79.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men's air max black shoe</p>\n",
       "                    <img src='data/footwear/7.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men's egoli white black shoe</p>\n",
       "                    <img src='data/footwear/22.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women ten blue white shoe</p>\n",
       "                    <img src='data/footwear/44.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women zoo blue shoe</p>\n",
       "                    <img src='data/footwear/43.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Adidas women's piona white shoe</p>\n",
       "                    <img src='data/footwear/26.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women main draw white blue shoe</p>\n",
       "                    <img src='data/footwear/28.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Hm women brown shoes</p>\n",
       "                    <img src='data/footwear/81.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the vector store with an unrelated query\n",
    "results = retriever.retrieve(\"Thank you!\")\n",
    "\n",
    "# Display results\n",
    "for item in results:\n",
    "    score = item.score\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(f\"Text: {item.get_content()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Visualize the results\n",
    "display_nodes_with_images_in_row(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32e025-f4b9-476b-9954-9c577cdd9b19",
   "metadata": {},
   "source": [
    "## Limitations of a Naive RAG System\n",
    "\n",
    "Regardless of the query, the vector database matches the closest vectors based on the embeddings. \n",
    "\n",
    "In this case, querying with \"Thank you!\" still returns shoes because the database doesn't understand the context or intent of the query.\n",
    "\n",
    "This demonstrates the limitation of a **naive RAG (Retrieval-Augmented Generation) system**. \n",
    "\n",
    "While it works well for focused queries like **\"red shoes\"**, it fails to adapt to non-specific or conversational inputs.\n",
    "\n",
    "Here's an illustration of this limitation:\n",
    "\n",
    "![Naive RAG System](images/1_dec/6_naive_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9dc5f-4d99-4e82-bc36-7d4c3854d4ca",
   "metadata": {},
   "source": [
    "## Create a Vector Index Query Engine\n",
    "\n",
    "To overcome the limitations of naive queries, we'll integrate an LLM into our workflow by creating a **Query Engine**.  \n",
    "\n",
    "This Query Engine will:\n",
    "1. Interpret the user's natural language input\n",
    "2. Retrieve contextually relevant information from the vector database\n",
    "3. Enable more dynamic and flexible interactions with the data\n",
    "\n",
    "For this guide, we'll use the `openai-o4` model for the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cd47c-3116-4cfd-94fb-cfb846cd8c62",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c32e8019-e035-45fd-92a8-e4fbf6f96683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = OpenAI_Llama(\n",
    "    temperature=0.0, \n",
    "    model=\"gpt-4o\", \n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba916e82-4e77-4ba3-9c71-c75914193a7d",
   "metadata": {},
   "source": [
    "## Create Query Engine\n",
    "\n",
    "We'll now create a Query Engine using the vector index and our custom embedding model. This engine will leverage the LLM for intelligent query interpretation and responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b040650-ea8f-4b73-9d96-96dce8289377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine from the vector index\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    embed_model=embed_model,\n",
    "    similarity_top_k=8,\n",
    "    vector_store_query_mode=\"hybrid\",\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231ebc0-593c-457d-9dfb-6ad510618a6c",
   "metadata": {},
   "source": [
    "## Test with Today's Challenge: Shoes for a Black-Tie Event\n",
    "\n",
    "Let's test the Query Engine by asking for shoes suitable for a black-tie event.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c38c619b-8230-4f0b-b4d1-fc81f7ede397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:  For a black-tie event, you would typically need formal shoes. The options provided are all casual or sports shoes, which may not be suitable for such an occasion.\n"
     ]
    }
   ],
   "source": [
    "# Query the engine for black-tie event shoes\n",
    "response = query_engine.query(\"I need shoes for a black-tie event\")\n",
    "\n",
    "print(\"Chatbot response: \", response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f959705-aa7e-4c28-b93a-e52bf1e69f1e",
   "metadata": {},
   "source": [
    "## Visualize pulled shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65640130-aff4-4557-9511-01178af2a636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Provogue men black shoes</p>\n",
       "                    <img src='data/footwear/4.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Skechers women black casual shoes</p>\n",
       "                    <img src='data/footwear/11.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Reebok women black casual shoes</p>\n",
       "                    <img src='data/footwear/13.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women flyclave black casual shoes</p>\n",
       "                    <img src='data/footwear/12.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Timberland men black casual shoes</p>\n",
       "                    <img src='data/footwear/3.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Rocia women multi- coloured shoes</p>\n",
       "                    <img src='data/footwear/69.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Adidas men eqt nitro fashion black sports shoes</p>\n",
       "                    <img src='data/footwear/1.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men's air max black shoe</p>\n",
       "                    <img src='data/footwear/7.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_nodes_with_images_in_row(response.source_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e0ed9-55f9-4e98-b0f0-aa4f99794e76",
   "metadata": {},
   "source": [
    "## Examine Results\n",
    "\n",
    "Looking at the results, the chatbot correctly understands the context of \"black-tie event\" as requiring formal shoes.  \n",
    "\n",
    "However, while the LLM correctly identifies that formal shoes are needed, it seems to retrieve casual or sports shoes due to the vector query. Here's the chatbot reply:\n",
    "\n",
    "> \"For a black-tie event, you would typically need formal shoes. The options provided are casual or sports shoes, which may not be suitable for such an occasion.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71577bd4-68b8-4682-8661-f185532493d5",
   "metadata": {},
   "source": [
    "### Why Did the Naive Chatbot Fail?\n",
    "\n",
    "The naive chatbot struggled with this query because it's designed to vectorize the **entire user query** and match it against the vector database. While this approach works for straightforward searches (e.g., \"red shoes\"), it has limitations for nuanced queries like \"shoes for a black-tie event.\"  \n",
    "\n",
    "**Key Limitations**:\n",
    "1. **Forced Query Vectorization**: By embedding the entire user query, the system treats every word in the query as equally important. This biases the search towards keywords like \"shoes\" or \"black,\" overlooking the broader intent of \"black-tie event.\"\n",
    "2. **Lack of Contextual Understanding**: The vector database operates without reasoning or context, so it retrieves products based solely on similarity to the query embedding, even if the retrieved results don't match the user's intent.\n",
    "\n",
    "In this case, the LLM recognized the mismatch (e.g., pulling casual or sports shoes for a formal event) but couldn't directly address it because the retrieval mechanism didn't align with the nuanced query.  \n",
    "\n",
    "Despite that black formal shoes exist in the dataset, the system failed to retrieve them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d5e103c8-921c-4643-9e47-0c2c36c56280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>color</th>\n",
       "      <th>color_details</th>\n",
       "      <th>usage</th>\n",
       "      <th>product_title</th>\n",
       "      <th>image</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>heels_height</th>\n",
       "      <th>titan_embedding</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>formal shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>formal</td>\n",
       "      <td>Lee cooper men black shoe</td>\n",
       "      <td>5.jpg</td>\n",
       "      <td>155</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>[-0.0017635934, -0.009471155, -0.014912436, -0...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>formal shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>formal</td>\n",
       "      <td>Arrow men formal black shoe</td>\n",
       "      <td>6.jpg</td>\n",
       "      <td>180</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>[0.013223984, -0.018667577, -0.024809424, -0.0...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id gender  category sub_category  product_type  color  \\\n",
       "4           5    men  footwear        shoes  formal shoes  black   \n",
       "5           6    men  footwear        shoes  formal shoes  black   \n",
       "\n",
       "  color_details   usage                product_title  image  price_usd  \\\n",
       "4            []  formal    Lee cooper men black shoe  5.jpg        155   \n",
       "5            []  formal  Arrow men formal black shoe  6.jpg        180   \n",
       "\n",
       "   heels_height                                    titan_embedding token_count  \n",
       "4          <NA>  [-0.0017635934, -0.009471155, -0.014912436, -0...           7  \n",
       "5          <NA>  [0.013223984, -0.018667577, -0.024809424, -0.0...           8  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the dataset for black formal shoes\n",
    "df_shoes[(df_shoes['product_type'] == 'formal shoes') & (df_shoes['color'] == 'black')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5b041-bce4-48d4-a48e-5f137830059e",
   "metadata": {},
   "source": [
    "## Query engine limitations\n",
    "\n",
    "This demonstrates another limitation: when the query embedding does not align perfectly with the database, important results may be missed.\n",
    "\n",
    "To address this, we'll now involve an AI Agent to enhance query handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ff7d5-5def-424e-af29-420b92facf73",
   "metadata": {},
   "source": [
    "## Create Vector Store Info\n",
    "\n",
    "We'll define metadata about the vector store to allow the AI agent to filter results based on specific attributes like gender, usage, and color. This metadata will enhance the agent's ability to refine queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "99b4f201-40ae-4421-a1ac-f47187112fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store information\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"shoes in the shoe store\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"gender\",\n",
    "            type=\"str\",\n",
    "            description=\"Either 'men' or 'women'\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"usage\",\n",
    "            type=\"str\",\n",
    "            description=\"Either 'sports', 'casual', or 'formal'\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"color\",\n",
    "            type=\"str\",\n",
    "            description=(\"Either 'black', 'white', 'blue', 'turquoise blue', 'red', 'pink', 'brown', 'green', or 'multi'\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8ada5-b118-4ac0-8e41-40212efb8a81",
   "metadata": {},
   "source": [
    "## Create Tools\n",
    "\n",
    "Tools are essential for enabling the AI Agent to interact with the vector store.  \n",
    "We'll define two tools:\n",
    "1. **`create_metadata_filter`**: Generates metadata filters for refining the search query\n",
    "2. **`search_footwear_database`**: Searches the vector database using the query and optional filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38a7c5-cec2-4b0f-a569-81473b73af57",
   "metadata": {},
   "source": [
    "## Define Metadata Filter Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "881cd56f-06f4-4b55-a29c-5cd265460194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool to create metadata filters\n",
    "def create_metadata_filter(filter_string):\n",
    "    \"\"\"\n",
    "    Creates metadata filter JSON for vector database queries.\n",
    "\n",
    "    Args:\n",
    "        filter_string (str): Query string for generating metadata filters.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON string of filters.\n",
    "    \"\"\"\n",
    "    class CustomRetriever(VectorIndexAutoRetriever):\n",
    "        def __init__(self, vector_index, vector_store_info, **kwargs):\n",
    "            super().__init__(vector_index, vector_store_info, **kwargs)\n",
    "\n",
    "        def _retrieve(self, query, **kwargs):\n",
    "            query_bundle = QueryBundle(query_str=query)\n",
    "            retrieval_spec = self.generate_retrieval_spec(query_bundle)\n",
    "            return retrieval_spec\n",
    "\n",
    "    custom_retriever = CustomRetriever(vector_index=vector_index, vector_store_info=vector_store_info)\n",
    "    retrieval_spec = custom_retriever._retrieve(filter_string)\n",
    "\n",
    "    filters_dicts = [{'key': f.key, 'value': f.value, 'operator': f.operator.value} for f in retrieval_spec.filters]\n",
    "    return json.dumps(filters_dicts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20794f-1a28-46ca-9030-b809ed93222b",
   "metadata": {},
   "source": [
    "## Define Footwear Vector Database Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f7621ef2-1efe-485f-bfee-d8ff9aaa03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool to search the footwear vector database\n",
    "def search_footwear_database(query_str, filters_json=None):\n",
    "    \"\"\"\n",
    "    Searches the footwear vector database using a query string and optional filters.\n",
    "\n",
    "    Args:\n",
    "        query_str (str): Query string describing the footwear.\n",
    "        filters_json (Optional[List]): JSON list of metadata filters.\n",
    "\n",
    "    Returns:\n",
    "        list: Search results from the vector database.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the embedding for the query string\n",
    "    query_embedding = embed_model._get_query_embedding(query_str)\n",
    "\n",
    "    # Deserialize from JSON\n",
    "    metadata_filters = MetadataFilters.from_dicts(filters_json, condition=FilterCondition.AND)\n",
    "    \n",
    "    vector_store_query = VectorStoreQuery(\n",
    "        query_str=query_str,\n",
    "        query_embedding=query_embedding,\n",
    "        alpha=0.5,\n",
    "        mode='hybrid',\n",
    "        filters=metadata_filters,\n",
    "        similarity_top_k=10\n",
    "    )\n",
    "    \n",
    "    # Execute the query against the vector store\n",
    "    query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "    # Create output without embeddings\n",
    "    nodes_with_scores = []\n",
    "    for index, node in enumerate(query_result.nodes):\n",
    "        score: Optional[float] = None\n",
    "        if query_result.similarities is not None:\n",
    "            score = query_result.similarities[index]\n",
    "        nodes_with_scores.append({\n",
    "            'color': node.metadata['color'],\n",
    "            'text': node.metadata['text'],\n",
    "            'gender': node.metadata['gender'],\n",
    "            'product_type': node.metadata['product_type'],\n",
    "            'product_id': node.metadata['product_id'],\n",
    "            'usage': node.metadata['usage'],\n",
    "            'price': node.metadata['price'],\n",
    "            'similarity_score': score\n",
    "        })\n",
    "\n",
    "    return nodes_with_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366ee68-aa49-48c9-b97e-580bdb56a686",
   "metadata": {},
   "source": [
    "## Define Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a1950185-56bb-4567-ae17-d989ff09da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metadata_filters_tool = FunctionTool.from_defaults(\n",
    "    name=\"create_metadata_filter\",\n",
    "    fn=create_metadata_filter\n",
    ")\n",
    "\n",
    "query_vector_database_tool = FunctionTool.from_defaults(\n",
    "    name=\"search_footwear_database\",\n",
    "    fn=search_footwear_database\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c703c-3841-4ab4-badc-49cfb4300344",
   "metadata": {},
   "source": [
    "## Create AI Agent\n",
    "\n",
    "We'll now define an AI Agent capable of reasoning over the data, generating filters, and performing refined searches to address customer queries more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77219b3-92c2-468d-a6ba-04dac8d88dfd",
   "metadata": {},
   "source": [
    "### Create the agent worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f69b032d-a57b-4c5e-b890-649e00d36d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent worker\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [\n",
    "        create_metadata_filters_tool,\n",
    "        query_vector_database_tool,\n",
    "    ],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    system_prompt=\"\"\"\\\n",
    "You are an agent designed to answer customers looking for shoes.\\\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "Drive sales and always feel free to ask a user for more information.\\\n",
    "\n",
    "- Always consider if filters are needed based on the user's query.\n",
    "- Use the tools provided to answer questions; do not rely on prior knowledge.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "User Query: \"Hi! I'm going to a party and I'm looking for red women's shoes. Thank you!\"\n",
    "\n",
    "Agent Actions:\n",
    "\n",
    "1. Determine what query string to use for filters e.g. \"red woman's shoes\"\n",
    "2. Call:\n",
    "   filter_string = create_metadata_filter_string(\"red woman's shoes\")\n",
    "3. Call:\n",
    "   results = search_footwear_database(query_str='shoes', filter_string=filter_string)\n",
    "\n",
    "Remember to follow these instructions carefully.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450ec80-0572-4d3c-991f-d2c2efc13e9f",
   "metadata": {},
   "source": [
    "### Create the agent runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "db72d671-b970-422a-a6c5-1231f79d8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12346a04-5a1a-4606-91a1-44bd5907808b",
   "metadata": {},
   "source": [
    "## Test Agent\n",
    "\n",
    "Let's test the AI Agent by asking for shoes suitable for a black-tie event.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3719234e-c0ae-48a0-8e1a-d2297e2ee8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: I need shoes for a black-tie event\n",
      "=== LLM Response ===\n",
      "To help you find the perfect shoes for a black-tie event, could you please specify if you are looking for men's or women's shoes? Additionally, do you have any preferences for color or style?\n"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "response = agent.chat(\"I need shoes for a black-tie event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0583c-0fd2-4e60-9b48-d5eae29ec1af",
   "metadata": {},
   "source": [
    "### Interpreting the Query\n",
    "\n",
    "Unlike a naive query engine, the AI agent approaches the request by interpreting the context and inferring the customer's true intent.  \n",
    "\n",
    "For the query **\"I need shoes for a black-tie event\"**, the agent doesn't immediately return a fixed set of vectors. Instead, it evaluates the context of the request and asks clarifying questions to ensure a precise recommendation.  \n",
    "\n",
    "Here's the agent's initial response:\n",
    "> **\"To help you find the perfect shoes for a black-tie event, could you please specify if you are looking for men's or women's shoes? Additionally, do you have any preferences for color or style?\"**\n",
    "\n",
    "By doing so, the agent ensures it fully understands the customer's needs before proceeding with a search:\n",
    "\n",
    "![Agent ensures it fully understands the customer's needs](images/1_dec/6_agent_clarification_question.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "24b2e762-59b2-4904-b3d6-b63b6b220c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Men's shoes for a black-tie event\n",
      "=== Calling Function ===\n",
      "Calling function: create_metadata_filter with args: {\"filter_string\": \"men's black-tie shoes\"}\n",
      "=== Function Output ===\n",
      "[{\"key\": \"gender\", \"value\": \"men\", \"operator\": \"==\"}]\n",
      "=== Calling Function ===\n",
      "Calling function: search_footwear_database with args: {\"query_str\": \"black-tie shoes\", \"filters_json\": [{\"key\": \"gender\", \"value\": \"men\", \"operator\": \"==\"}]}\n",
      "=== Function Output ===\n",
      "[{'color': 'black', 'text': 'Provogue men black shoes', 'gender': 'men', 'product_type': 'casual shoes', 'product_id': 4, 'usage': 'casual', 'price': 125, 'similarity_score': 2.22137713}, {'color': 'black', 'text': 'Timberland men black casual shoes', 'gender': 'men', 'product_type': 'boots', 'product_id': 3, 'usage': 'casual', 'price': 60, 'similarity_score': 2.20507097}, {'color': 'black', 'text': 'Adidas men eqt nitro fashion black sports shoes', 'gender': 'men', 'product_type': 'sports shoes', 'product_id': 1, 'usage': 'sports', 'price': 120, 'similarity_score': 2.18479443}, {'color': 'black', 'text': \"Nike men's air max black shoe\", 'gender': 'men', 'product_type': 'sports shoes', 'product_id': 7, 'usage': 'sports', 'price': 80, 'similarity_score': 1.75484717}, {'color': 'white', 'text': \"Nike men's egoli white black shoe\", 'gender': 'men', 'product_type': 'sports shoes', 'product_id': 22, 'usage': 'sports', 'price': 200, 'similarity_score': 1.73927808}, {'color': 'black', 'text': 'Arrow men formal black shoe', 'gender': 'men', 'product_type': 'formal shoes', 'product_id': 6, 'usage': 'formal', 'price': 180, 'similarity_score': 1.73427236}, {'color': 'brown', 'text': 'Enroute men leather brown formal shoes', 'gender': 'men', 'product_type': 'formal shoes', 'product_id': 73, 'usage': 'formal', 'price': 70, 'similarity_score': 1.72893286}, {'color': 'brown', 'text': 'Force 10 men brown shoes', 'gender': 'men', 'product_type': 'casual shoes', 'product_id': 70, 'usage': 'sports', 'price': 55, 'similarity_score': 1.72278774}, {'color': 'blue', 'text': 'Skechers men blue shoes', 'gender': 'men', 'product_type': 'casual shoes', 'product_id': 34, 'usage': 'casual', 'price': 195, 'similarity_score': 1.72093236}, {'color': 'brown', 'text': 'Enroute men leather brown formal shoes', 'gender': 'men', 'product_type': 'formal shoes', 'product_id': 74, 'usage': 'formal', 'price': 70, 'similarity_score': 1.7174381}]\n",
      "=== LLM Response ===\n",
      "Here are some men's shoes suitable for a black-tie event:\n",
      "\n",
      "1. **Arrow Men Formal Black Shoe**\n",
      "   - Type: Formal Shoes\n",
      "   - Color: Black\n",
      "   - Price: $180\n",
      "\n",
      "2. **Enroute Men Leather Brown Formal Shoes**\n",
      "   - Type: Formal Shoes\n",
      "   - Color: Brown\n",
      "   - Price: $70\n",
      "\n",
      "These options are formal and would complement a black-tie attire. If you have any specific preferences or need further assistance, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "# Specify preferences in the query\n",
    "agent_response = agent.chat(\"Men's shoes for a black-tie event\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1292fe-6b8b-42be-83de-ee3c53209abd",
   "metadata": {},
   "source": [
    "# Detailed Agent Workflow\n",
    "\n",
    "## Agent Workflow\n",
    "\n",
    "When refining the query to \"Men's shoes for a black-tie event,\" the agent takes the following steps:\n",
    "\n",
    "1. **Contextual Understanding**:\n",
    "   - Recognizes the query's requirement for formal men's shoes suitable for a black-tie event.\n",
    "2. **Tool Invocation**:\n",
    "   - Calls the `create_metadata_filter` function with the argument `filter_string=\"men's black-tie shoes\"`, generating a metadata filter:\n",
    "     ```json\n",
    "     [{\"key\": \"gender\", \"value\": \"men\", \"operator\": \"==\"}]\n",
    "     ```\n",
    "   - Calls the `search_footwear_database` function with the query string \"black-tie shoes\" and the generated filter. This refines the search to include only men's shoes.\n",
    "3. **Response Generation**:\n",
    "   - Combines the retrieved vector database results with its contextual understanding to provide a natural language response.\n",
    "\n",
    "Here's the agent's final response:\n",
    "> \"Here are some options for men's shoes suitable for a black-tie event:  \n",
    ">  1. **Arrow Men Formal Black Shoe**  \n",
    ">    - Color: Black  \n",
    ">    - Type: Formal Shoes  \n",
    ">    - Price: \\$180\n",
    ">  \n",
    ">  2. **Enroute Men Leather Brown Formal Shoes**  \n",
    ">    - Color: Brown  \n",
    ">    - Type: Formal Shoes  \n",
    ">    - Price: \\$70\n",
    ">  \n",
    "> These formal shoes would be appropriate for a black-tie event. If you have any specific preferences or need further assistance, feel free to let me know!\"\n",
    "\n",
    "This workflow highlights the agent's ability to bridge gaps between the customer's query and the vector database results by applying reasoning and metadata filters.\n",
    "\n",
    "![Agent workflow](images/1_dec/6_agent_clarification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "686dde27-77a1-4281-8bf5-77d462e9bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_shoes = ['Arrow Men Formal Black Shoe', 'Enroute Men Leather Brown Formal Shoes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2fd52-0af9-4b65-bfd4-61adb3dc77cb",
   "metadata": {},
   "source": [
    "# Visualize the Agent's Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b85a7f73-aada-4fce-bd0d-8e1c54229681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "            <div style=\"text-align: center;\">\n",
       "                <p>Arrow men formal black shoe</p>\n",
       "                <img src='data/footwear/6.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "            </div>\n",
       "        \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p>Enroute men leather brown formal shoes</p>\n",
       "                <img src='data/footwear/73.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "            </div>\n",
       "        \n",
       "            <div style=\"text-align: center;\">\n",
       "                <p>Enroute men leather brown formal shoes</p>\n",
       "                <img src='data/footwear/74.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "            </div>\n",
       "        </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_folder=\"data/footwear\"\n",
    "img_width=150\n",
    "html_content = \"<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\"\n",
    "\n",
    "for node in agent_response.sources[1].raw_output:\n",
    "\n",
    "    if any(node['text'].lower() in shoe.lower() for shoe in recommended_shoes):\n",
    "        # Generate image path based on product_id\n",
    "        image_path = os.path.join(image_folder, f\"{node['product_id']}.jpg\")\n",
    "        html_content += f\"\"\"\n",
    "            <div style=\"text-align: center;\">\n",
    "                <p>{node['text']}</p>\n",
    "                <img src='{image_path}' width='{img_width}px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "# Close the main div\n",
    "html_content += \"</div>\"\n",
    "\n",
    "# Display the content as HTML\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c4671-7cb1-4eb1-b56d-9f665e6d9c07",
   "metadata": {},
   "source": [
    "## Why the AI Agent Succeeds\n",
    "\n",
    "This example illustrates how the AI agent overcomes the limitations of naive RAG systems and simple LLM-powered query engines:\n",
    "\n",
    "1. **Clarifies Ambiguous Queries**:  \n",
    "   By asking follow-up questions, the agent ensures it understands the user's preferences (e.g., gender, style) before proceeding.\n",
    "   \n",
    "2. **Refines Search with Metadata Filters**:  \n",
    "   It generates and applies metadata filters to focus the search results on the most relevant subset of the vector database. This step bridges the gap between broad vector queries and the customer's specific intent.\n",
    "\n",
    "3. **Combines Contextual Reasoning with Data**:  \n",
    "   The agent synthesizes its understanding of \"black-tie event\" with the vector database results to provide accurate and human-like recommendations.\n",
    "\n",
    "The AI agent's workflow highlights the value of integrating reasoning, filtering, and retrieval for advanced customer queries. It not only retrieves suitable shoes but also explains its reasoning, making the interaction more interactive and customer-friendly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9a0f0-2b1e-4c25-85c6-bedbf2b2bd49",
   "metadata": {},
   "source": [
    "## Conclusion: Day 1 - How AI Agents Improve Naive Chatbots by Asking Clarifying Questions\n",
    "\n",
    "Today, we explored one of the most common limitations of naive Retrieval-Augmented Generation (RAG) systems: their inability to handle ambiguous queries effectively.  \n",
    "\n",
    "Through our example of searching for shoes for a black-tie event, we demonstrated how:\n",
    "1. Naive chatbots fall short by blindly vectorizing the entire user query and relying solely on embedding similarity.\n",
    "2. AI agents enhance the experience by:\n",
    "   - Interpreting the intent behind user queries\n",
    "   - Asking clarifying questions to fill in missing details\n",
    "   - Applying metadata filters to refine search results\n",
    "   - Generating responses that are both accurate and customer-centric\n",
    "\n",
    "By focusing on **contextual reasoning and dynamic interaction**, AI agents transform customer queries into meaningful, actionable results.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Tomorrow, we'll continue our journey by tackling another challenge faced by naive chatbots.\n",
    "\n",
    "---\n",
    "\n",
    "### Ready to implement this in your own systems?  \n",
    "AI agents are the future of e-commerce chatbots. If you're interested in applying these concepts to your business, feel free to reach out!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b110a1-e6e7-4093-9701-ce3825789d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf82f75-230c-4197-9e9e-a3e006dcd59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d7955-5d65-4f02-9975-435a61aa1b08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
