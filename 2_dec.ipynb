{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c77d554-ddd8-447e-804b-8b0bf20c2409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from IPython.display import display, Image, HTML\n",
    "\n",
    "# LlamaIndex\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# LlamaIndex agents\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker, AgentRunner\n",
    "\n",
    "# LlamaIndex LLMs\n",
    "from llama_index.llms.openai import OpenAI as OpenAI_Llama\n",
    "\n",
    "# LlamaIndex metadata filters\n",
    "from llama_index.core.vector_stores.types import (\n",
    "    MetadataFilters,FilterCondition\n",
    ")\n",
    "\n",
    "# LlamaIndex retrievers\n",
    "from llama_index.core.retrievers import VectorIndexAutoRetriever, VectorIndexRetriever\n",
    "\n",
    "# LlamaIndex vector stores\n",
    "from llama_index.core.vector_stores import MetadataInfo, VectorStoreInfo\n",
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "from llama_index.core.vector_stores.types import VectorStoreQuery\n",
    "\n",
    "# Pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "import pandas as pd\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm import tqdm\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf5994f-4da4-40af-9511-16fa120a3a1d",
   "metadata": {},
   "source": [
    "# Advent Calendar Day 2: How AI Agents Improve Naive Chatbots by Understanding Context Shifts\n",
    "\n",
    "This December, we're highlighting the limitations of simple AI chatbots in online retail and demonstrating how **AI agents** enhance customer interactions.\n",
    "\n",
    "Each day, we'll explore a common challenge faced by naive Retrieval Augmented Generation (RAG) chatbot systems and show how AI agents overcome them. \n",
    "\n",
    "Todays topic is about how AI agents improve naive chatbots by understanding context shifts.\n",
    "\n",
    "![Cover image](images/2_dec/cover.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68ece46-df06-480c-a3f0-2a3e6c8df93b",
   "metadata": {},
   "source": [
    "## Introducing SoleMates\n",
    "\n",
    "***SoleMates*** is a fictional online shoe store that we'll use as a practical example throughout this tutorial.\n",
    "\n",
    "We'll explore interactions between customers and chatbots at SoleMates, highlighting the differences between basic chatbots and advanced AI agents.\n",
    "\n",
    "![SoleMates Illustration](images/solemates.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ee172-c3a7-4ca5-a1a8-49589b4faa1e",
   "metadata": {},
   "source": [
    "## Today's Challenge: Failure to Adapt After Context Shift\n",
    "\n",
    "### Scenario\n",
    "\n",
    "A customer initiates a chat with **SoleMates**:\n",
    "\n",
    "**Customer:** \"Hi! Iâ€™m looking for women's casual shoes\"\n",
    "\n",
    "![A customer initiates a chat with SoleMates](images/2_dec/1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed45719-0e50-4583-a6c3-f147fe888b86",
   "metadata": {},
   "source": [
    "# Step-by-step walkthrough\n",
    "Let's build a simple chatbot that can answer this, step-by-step together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b450882-00c9-4262-a421-b9be0dd81638",
   "metadata": {},
   "source": [
    "## Load Shoe Data\n",
    "\n",
    "Let's start by reading the SoleMates shoe dataset. This dataset contains detailed product information, such as shoe colors and heel heights, which we'll transform into embeddings and store in a cloud-based Pinecone vector database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b37126db-d68a-4a3c-94f1-9a04471894c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>color</th>\n",
       "      <th>color_details</th>\n",
       "      <th>usage</th>\n",
       "      <th>product_title</th>\n",
       "      <th>image</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>heels_height</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>sports shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[neon green]</td>\n",
       "      <td>sports</td>\n",
       "      <td>Adidas men eqt nitro fashion black sports shoes</td>\n",
       "      <td>1.jpg</td>\n",
       "      <td>120</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>adidas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>sports shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[white]</td>\n",
       "      <td>sports</td>\n",
       "      <td>Puma men's yugorun black white shoe</td>\n",
       "      <td>2.jpg</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>puma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>boots</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>casual</td>\n",
       "      <td>Timberland men black casual shoes</td>\n",
       "      <td>3.jpg</td>\n",
       "      <td>60</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>timberland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>casual shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>casual</td>\n",
       "      <td>Provogue men black shoes</td>\n",
       "      <td>4.jpg</td>\n",
       "      <td>125</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>provogue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>men</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>formal shoes</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>formal</td>\n",
       "      <td>Lee cooper men black shoe</td>\n",
       "      <td>5.jpg</td>\n",
       "      <td>155</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>lee cooper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id gender  category sub_category  product_type  color  \\\n",
       "0           1    men  footwear        shoes  sports shoes  black   \n",
       "1           2    men  footwear        shoes  sports shoes  black   \n",
       "2           3    men  footwear        shoes         boots  black   \n",
       "3           4    men  footwear        shoes  casual shoes  black   \n",
       "4           5    men  footwear        shoes  formal shoes  black   \n",
       "\n",
       "  color_details   usage                                    product_title  \\\n",
       "0  [neon green]  sports  Adidas men eqt nitro fashion black sports shoes   \n",
       "1       [white]  sports              Puma men's yugorun black white shoe   \n",
       "2            []  casual                Timberland men black casual shoes   \n",
       "3            []  casual                         Provogue men black shoes   \n",
       "4            []  formal                        Lee cooper men black shoe   \n",
       "\n",
       "   image  price_usd  heels_height       brand  \n",
       "0  1.jpg        120          <NA>      adidas  \n",
       "1  2.jpg         50          <NA>        puma  \n",
       "2  3.jpg         60          <NA>  timberland  \n",
       "3  4.jpg        125          <NA>    provogue  \n",
       "4  5.jpg        155          <NA>  lee cooper  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the SoleMates shoe dataset\n",
    "df_shoes = pd.read_csv('data/solemates_shoe_directory.csv')\n",
    "\n",
    "# Convert 'color_details' from string representation of a list to an actual list\n",
    "df_shoes['color_details'] = df_shoes['color_details'].apply(ast.literal_eval)\n",
    "\n",
    "# Ensure 'heels_height' is treated as a nullable integer type\n",
    "df_shoes['heels_height'] = df_shoes['heels_height'].astype('Int64')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df_shoes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b45b36-59b2-4aff-bafe-c95eb16c731f",
   "metadata": {},
   "source": [
    "## Cost of Vectorization and Pre-Embedded Dataset\n",
    "\n",
    "Vectorizing datasets with AWS Bedrock and the Titan multimodal model involves costs based on the number of input tokens and images:\n",
    "\n",
    "- **Text embeddings**: $0.0008 per 1,000 input tokens  \n",
    "\n",
    "- **Image embeddings**: $0.00006 per image  \n",
    "\n",
    "The provided SoleMates dataset is small, containing just 88 pairs of shoes, making it affordable to vectorize. For this dataset, I calculated the total cost of vectorization and summarized the token counts below:\n",
    "\n",
    "- **Token Count**: `858` tokens  \n",
    "- **Total Cost**: `$0.006`  \n",
    "\n",
    "If you prefer not to generate embeddings yourself or don't have access to AWS, you can use a pre-embedded dataset that I've prepared as a CSV file. This file includes all embeddings and token counts, allowing you to follow the guide without incurring additional costs. However, for hands-on experience, I recommend running the embedding process to understand the workflow.\n",
    "\n",
    "To load the pre-embedded dataset, use the following code:\n",
    "```python\n",
    "# Load pre-embedded dataset\n",
    "df_shoes = pd.read_csv('data/solemates_shoe_directory_pre_embedded_shoes.csv')\n",
    "```\n",
    "This step is entirely optional and designed to accommodate various levels of access and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53134c90-80fc-4ba2-a75f-041dda15a60e",
   "metadata": {},
   "source": [
    "### Prepare Amazon Bedrock for Embedding Generation\n",
    "\n",
    "To vectorize our product data, we'll generate embeddings for each product using AWS Titan. These embeddings combine image and text data to represent each product in a format suitable for search and recommendation systems.\n",
    "\n",
    ">**Important Note on Cost**:  \n",
    ">Vectorizing datasets incurs a cost. The SoleMates dataset contains 88 pairs of shoes, resulting in an estimated total cost of `$0.006`.\n",
    ">\n",
    ">I've added a token count column to help track these costs, and you can calculate your own total for larger datasets.\n",
    "\n",
    "If you'd rather not generate embeddings yourself, you can load a pre-embedded version of the dataset I've provided. This is entirely optional but ensures you can still follow along with the guide:\n",
    "```python\n",
    "# Load pre-embedded dataset\n",
    "df_shoes = pd.read_csv('data/solemates_shoe_directory_pre_embedded_shoes.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5937c5f-52a7-454d-af40-d2930888164c",
   "metadata": {},
   "source": [
    "## Getting started with Amazon Bedrock\n",
    "To use Amazon Bedrock for embedding generation, start by setting up your AWS environment:\n",
    "\n",
    "1. Create an AWS account if you don't already have one\n",
    "2. Set up an AWS Identity and Access Management (IAM) role with permissions tailored for Amazon Bedrock\n",
    "3. Submit a request to access the foundation models (FMs) you'd like to use\n",
    "\n",
    "Next, we'll initialize the Bedrock runtime client, which allows us to interact with AWS Titan for embedding generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773bd1f3-74f1-43a5-8c72-1a008ef5f96e",
   "metadata": {},
   "source": [
    "## Set up AWS Bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd1c643a-0e10-49f8-bc89-0f5284c376de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your AWS profile \n",
    "# Replace 'your-profile-name' with the name of your AWS CLI profile\n",
    "# To use your default AWS profile, leave 'aws_profile' as None\n",
    "aws_profile = os.environ.get('AWS_PROFILE')\n",
    "\n",
    "# Specify the AWS region where Bedrock is available\n",
    "aws_region_name = \"us-east-1\"\n",
    "\n",
    "try:\n",
    "    # Set the default session for the specified profile\n",
    "    if aws_profile:\n",
    "        boto3.setup_default_session(profile_name=aws_profile)\n",
    "    else:\n",
    "        boto3.setup_default_session()  # Use default AWS profile if none is specified\n",
    "    \n",
    "    # Initialize the Bedrock runtime client\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=aws_region_name\n",
    "    )\n",
    "except NoCredentialsError:\n",
    "    print(\"AWS credentials not found. Please configure your AWS profile.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469208e5-3388-4f70-8c2a-c0399a6904a3",
   "metadata": {},
   "source": [
    "## Generate Embeddings for Product Data\n",
    "\n",
    "To prepare our product data for the vector database, we'll generate embeddings for each product using AWS Titan. These embeddings combine image and text data to represent each product in a format suitable for search and recommendation systems.\n",
    "\n",
    "Before generating embeddings, we'll initialize two new columns in the dataset:\n",
    "- **`titan_embedding`**: To store the embedding vectors\n",
    "- **`token_count`**: To store the token count for each product title\n",
    "\n",
    "Then, we'll define a function to generate embeddings and apply it to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003c254-97ca-486d-bf2b-859f0a0a8412",
   "metadata": {},
   "source": [
    "## Initialize Columns for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74eef96-fef3-4bfe-ac6c-0788e8dd3884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize columns to store embeddings and token counts\n",
    "df_shoes['titan_embedding'] = None  # Placeholder for embedding vectors\n",
    "df_shoes['token_count'] = None  # Placeholder for token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b450e6af-4cdb-4607-927f-d634b4250674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to generate image and text embeddings\n",
    "def generate_embeddings(df, image_col='image', text_col='product_title', embedding_col='embedding', image_folder='data/footwear'):\n",
    "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Generating embeddings\"):\n",
    "        try:\n",
    "            # Prepare image file as base64\n",
    "            image_path = os.path.join(image_folder, row[image_col])\n",
    "            with open(image_path, 'rb') as img_file:\n",
    "                image_base64 = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "            \n",
    "            # Create input data for the model\n",
    "            input_data = {\"inputImage\": image_base64, \"inputText\": row[text_col]}\n",
    "\n",
    "            # Invoke AWS Titan model via Bedrock runtime\n",
    "            response = bedrock_runtime.invoke_model(\n",
    "                body=json.dumps(input_data),\n",
    "                modelId=\"amazon.titan-embed-image-v1\",\n",
    "                accept=\"application/json\",\n",
    "                contentType=\"application/json\"\n",
    "            )\n",
    "            response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "            # Extract embedding and token count from response\n",
    "            embedding = response_body.get(\"embedding\")\n",
    "            token_count = response_body.get(\"inputTextTokenCount\")\n",
    "\n",
    "            # Validate and save the embedding\n",
    "            if isinstance(embedding, list):\n",
    "                df.at[index, embedding_col] = embedding  # Save embedding as a list\n",
    "                df.at[index, 'token_count'] = int(token_count)  # Save token count as an integer\n",
    "            else:\n",
    "                raise ValueError(\"Embedding is not a list as expected.\")\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Error for row {index}: {e}\")\n",
    "            df.at[index, embedding_col] = None  # Handle errors gracefully\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca417c9-8156-4653-87da-d54008e350e4",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51db4597-71ba-4aa3-813d-56d97669579f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1f4dbbacca4fa6a9738420ca6ede15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate embeddings for the product data\n",
    "df_shoes = generate_embeddings(df=df_shoes, embedding_col='titan_embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b828c6-886f-481f-9f15-3093f9f2ef76",
   "metadata": {},
   "source": [
    "## Save Dataset for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ec066-c5e1-4364-8acd-65d84ce220ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset with generated embeddings to a CSV file\n",
    "# Get today's date in YYYY_MM_DD format\n",
    "today = datetime.now().strftime('%Y_%m_%d')\n",
    "\n",
    "# Save the dataset with generated embeddings to a CSV file\n",
    "df_shoes.to_csv(f'shoes_with_embeddings_token_{today}.csv', index=False)\n",
    "print(f\"Dataset with embeddings saved as 'shoes_with_embeddings_token_{today}.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f0538-ee08-4f42-8e6c-693bc480da80",
   "metadata": {},
   "source": [
    "## Create a Dictionary with Product Data\n",
    "\n",
    "Before we create LlamaIndex `Document` objects, we need to structure the product data into dictionaries. These dictionaries include:\n",
    "\n",
    "1. **Text**: The product title that will be used for embedding queries.\n",
    "2. **Metadata**: A dictionary containing detailed attributes for each product (e.g., color, gender, usage, price).\n",
    "3. **Embedding**: The Titan embeddings generated earlier.\n",
    "\n",
    "This dictionary format ensures the data is well-organized for creating `Document` objects in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f2408c-86fe-437c-96e8-ee42718bfd63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame rows into a list of dictionaries for LlamaIndex\n",
    "product_data = df_shoes.apply(lambda row: {\n",
    "    'text': row['product_title'],\n",
    "    'metadata': {\n",
    "        'color': row['color'],\n",
    "        'text': row['product_title'],\n",
    "        'gender': row['gender'],\n",
    "        'product_type': row['product_type'],\n",
    "        'usage': row['usage'],\n",
    "        'price': row['price_usd'],\n",
    "        'product_id': row['product_id'],\n",
    "        'brand': row['brand'],\n",
    "        **({'heels_height': int(row['heels_height'])} if not pd.isna(row['heels_height']) else {}),\n",
    "        **({'color_details': row['color_details']} if row['color_details'] else {})\n",
    "    },\n",
    "    'embedding': row['titan_embedding']\n",
    "}, axis=1).tolist()\n",
    "\n",
    "# Preview the first product dictionary\n",
    "#product_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7e75d-38c0-4a21-a064-ad24ffc13a50",
   "metadata": {},
   "source": [
    "## Create LlamaIndex Documents\n",
    "\n",
    "We'll now use the product data dictionaries to create LlamaIndex `Document` objects. \n",
    "\n",
    "These `Documents` are crucial because:\n",
    "\n",
    "- They act as containers for our product data and embeddings.\n",
    "- They enable seamless interaction with Pinecone for upserting embeddings.\n",
    "\n",
    "Each `Document` includes:\n",
    "1. The **text** (product title) for embedding and query purposes\n",
    "2. **Metadata** with attributes like color, gender, and price\n",
    "3. The **embedding** generated earlier\n",
    "4. An **exclusion list** (`excluded_embed_metadata_keys`) to prevent unnecessary metadata fields from being embedded, ensuring optimal performance and cost-efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbd4dbb-046b-4ccc-81ae-76f73c7ab5a6",
   "metadata": {},
   "source": [
    "## Create LlamaIndex Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150978c8-c65f-4bfc-a6e7-b96972e35b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'color': 'black',\n",
       " 'text': 'Adidas men eqt nitro fashion black sports shoes',\n",
       " 'gender': 'men',\n",
       " 'product_type': 'sports shoes',\n",
       " 'usage': 'sports',\n",
       " 'price': 120,\n",
       " 'product_id': 1,\n",
       " 'brand': 'adidas',\n",
       " 'color_details': ['neon green']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create LlamaIndex Document objects\n",
    "documents = []\n",
    "for doc in product_data:\n",
    "    documents.append(\n",
    "        Document(\n",
    "            text=doc[\"text\"],\n",
    "            extra_info=doc[\"metadata\"],\n",
    "            embedding=doc['embedding'],\n",
    "            \n",
    "            # Avoid embedding unnecessary metadata\n",
    "            excluded_embed_metadata_keys=[\n",
    "                'color',\n",
    "                'gender',\n",
    "                'product_type',\n",
    "                'usage',\n",
    "                'text',\n",
    "                'price',\n",
    "                'product_id',\n",
    "                'brand',\n",
    "                'heels_height',\n",
    "                'color_details'\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Confirm the first Document object\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e952186-1753-4d4d-a3a2-70ba897ac888",
   "metadata": {},
   "source": [
    "## Initialize Pinecone\n",
    "\n",
    "To interact with Pinecone, you'll first need an account and API keys. If you don't already have them, [create a Pinecone account](https://www.pinecone.io/) and retrieve your API key.\n",
    "\n",
    "Pinecone is a vector database designed to store and query embeddings. We'll use Pinecone to upsert the AWS Titan embeddings we generated earlier, enabling efficient similarity and hybrid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9aad5db-4754-45bb-86dd-6f2a6d4206bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone client with API key\n",
    "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
    "index_name = \"solemates\"  # Replace with your desired index name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41932e99-12f7-420e-ba70-de784970fa7b",
   "metadata": {},
   "source": [
    "## List Current Indexes\n",
    "\n",
    "Let's list the existing indexes in your Pinecone account to ensure no duplicates before creating a new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68064603-74ca-4950-bcef-74c03cd97b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': []}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List current indexes\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fff0f-c4fb-4c0f-8773-4be229abf3dd",
   "metadata": {},
   "source": [
    "## Create Index\n",
    "\n",
    "Next, we'll create a Pinecone index. An index stores the embeddings and metadata for your data.\n",
    "\n",
    "- **Dimension**: Matches the size of the embeddings we're using (1024 for AWS Titan multimodal embeddings)\n",
    "- **Metric**: Defines how similarity is calculated (e.g., dot product, cosine similarity)\n",
    "- **ServerlessSpec**: Specifies the cloud provider and region for your index\n",
    "\n",
    "If the index already exists, this step will be skipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae7c9a7-3d63-4804-b667-6ef15e014347",
   "metadata": {},
   "outputs": [],
   "source": [
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,  # AWS Titan embeddings require 1024 dimensions\n",
    "        metric=\"dotproduct\",  # Required for hybrid search with Pinecone\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f37e20-afe2-4695-a5db-2e5202ea0f7f",
   "metadata": {},
   "source": [
    "## Inspect Pinecone Index\n",
    "\n",
    "Navigate to your Pinecone dashboard, and you should now see your new index with **0 records (vectors)**, as it hasn't been populated yet:\n",
    "\n",
    "![Pinecone shows an empty index](images/pinecone/4_pinecone_empty_index.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc931ec8-278e-4be1-bb5b-cc1cd55d072f",
   "metadata": {},
   "source": [
    "## Initialize Pinecone Index\n",
    "\n",
    "After creating the index, we'll initialize it for further operations like upserting embeddings and querying vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d63f315-49f2-402b-8b19-424e60d23438",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a34df-bd42-4a16-a850-faa153291855",
   "metadata": {},
   "source": [
    "## Create Pinecone Vector Store\n",
    "\n",
    "We'll now set up a **Pinecone Vector Store** using LlamaIndex. \n",
    "\n",
    "This vector store connects our Pinecone index with the LlamaIndex framework.\n",
    "\n",
    "Key configuration details:\n",
    "1. **Namespace**: A logical grouping within the index, allowing future addition of other product types\n",
    "2. **Hybrid Search**: Enabling both semantic and keyword search by adding sparse vectors\n",
    "\n",
    "For more information:\n",
    "- [Pinecone Namespaces Guide](https://docs.pinecone.io/guides/indexes/use-namespaces)\n",
    "- [Hybrid Search Introduction](https://www.pinecone.io/learn/hybrid-search-intro/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1332ed46-d9e5-4048-8817-b7242b69cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(\n",
    "    pinecone_index=pinecone_index,\n",
    "    namespace='footwear',  # Logical namespace for shoe data\n",
    "    add_sparse_vector=True  # Enables hybrid search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91796b22-3028-4e63-98bc-83384506fb2f",
   "metadata": {},
   "source": [
    "## Create an Ingestion Pipeline\n",
    "\n",
    "We'll create an **Ingestion Pipeline** to upsert our vectors into the Pinecone index. \n",
    "No transformations are required since we've pre-generated embeddings with AWS Titan.\n",
    "\n",
    ">**Note**: As of Dec 4 2024, LlamaIndex doesn't abstract AWS Titan multimodal embeddings, so we're using our own vectors directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b59b4745-0392-4426-9db7-f96ef51133c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = IngestionPipeline(\n",
    "    transformations=[],  # No transformations since embeddings are pre-generated\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b133b0-e886-48fd-984b-6851f8e22f27",
   "metadata": {},
   "source": [
    "## Run the Ingestion Pipeline\n",
    "\n",
    "This step upserts the embeddings into Pinecone for storage and querying.\n",
    "\n",
    "- **Cost Note**: Pinecone charges $2.00 per 1M vectors unless you're on the free plan\n",
    "- **Time Note**: It may take a minute or two for the vectors to become visible in your Pinecone index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6475df3-a622-4b7a-9a6b-e4668b32b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline to upsert embeddings into Pinecone\n",
    "pipeline.run(documents=documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace33ed-ff9b-451a-8735-c05f3e6e0c23",
   "metadata": {},
   "source": [
    "## Inspect Pinecone Index\n",
    "\n",
    "Now that we've upserted the vectors, navigate back to Pinecone. You should see **88 records** in your index, corresponding to the embeddings we added:\n",
    "\n",
    "![Populated Pinecone index](images/pinecone/5_pinecone_populated_index.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3916b2-9897-47dd-8946-e824b962ea5b",
   "metadata": {},
   "source": [
    "## Test Querying the Vector Store\n",
    "\n",
    "Now that we have upserted all our shoe vectors, let's test querying the vector database.  \n",
    "\n",
    "We'll start by creating a **Vector Store Index** with LlamaIndex. This index will allow us to query the Pinecone index using the same vector store we initialized earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "909e3af8-b9bd-47aa-9b0f-c0837bb95323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Vector Store Index\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828376ed-f6c6-4f86-8216-ee7f28396ade",
   "metadata": {},
   "source": [
    "## Query the Vector Database Directly (Without Query Engine or Chat Engine)\n",
    "\n",
    "Before we use a **Query Engine** or **Chat Engine** to interact with the vector database, we'll start with a direct query using a simple retriever.  \n",
    "\n",
    "This approach demonstrates how you can fetch relevant records from the database without involving advanced reasoning, natural language understanding, or conversation tracking. It's a fundamental way to confirm that the embeddings and metadata are stored correctly and the vector database is functioning as expected.\n",
    "\n",
    "Next, we'll move on to more advanced querying techniques, including using a **Query Engine** and an **Agent** to leverage the power of LLMs.\n",
    "\n",
    "The first step is creating a simple retriever, but first, we need to define a custom embedding function. \n",
    "\n",
    "As of Dec 4, 2024, **LlamaIndex does not abstract AWS Titan multimodal embeddings**, so we'll implement a custom class for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327db3ff-fa36-4f98-b93c-4f805cfc9f25",
   "metadata": {},
   "source": [
    "## Create a Function to Request AWS Titan Embeddings\n",
    "\n",
    "We'll define a helper function to request embeddings from AWS Titan's multimodal model. This function will handle both text and image inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5be13cb5-c6e8-4cb0-a547-e5cb9e6b9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_embedding(image_base64=None, text_description=None):\n",
    "    \"\"\"\n",
    "    Request embeddings from AWS Titan multimodal model.\n",
    "\n",
    "    Parameters:\n",
    "        image_base64 (str, optional): Base64 encoded image string.\n",
    "        text_description (str, optional): Text description.\n",
    "\n",
    "    Returns:\n",
    "        list: Embedding vector.\n",
    "    \"\"\"\n",
    "    input_data = {\"inputImage\": image_base64, \"inputText\": text_description}\n",
    "    body = json.dumps(input_data)\n",
    "\n",
    "    # Invoke the Titan multimodal model\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=\"amazon.titan-embed-image-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    if response_body.get(\"message\"):\n",
    "        raise ValueError(f\"Embeddings generation error: {response_body.get('message')}\")\n",
    "\n",
    "    return response_body.get(\"embedding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124454d7-a2bb-4950-9869-07faca07ecaa",
   "metadata": {},
   "source": [
    "## Create Custom Embeddings Class\n",
    "\n",
    "We'll now define a custom embedding class that uses the AWS Titan multimodal model to fetch embeddings. \n",
    "\n",
    "This class overrides key methods in LlamaIndex's `BaseEmbedding` to integrate AWS Titan into the framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a699d370-4f5b-4b66-9a62-6e50bcfb98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEmbeddings(BaseEmbedding):\n",
    "    \"\"\"\n",
    "    Custom embedding class for AWS Titan multimodal embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"multimodal\"\n",
    "    \n",
    "    async def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_query_embedding(query)\n",
    "\n",
    "    async def _aget_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_text_embedding(text)\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get embeddings for a query string.\n",
    "        \"\"\"\n",
    "        return request_embedding(text_description=query)\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get embeddings for a text string.\n",
    "        \"\"\"\n",
    "        return request_embedding(text_description=text)\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Get embeddings for a batch of text strings.\n",
    "        \"\"\"\n",
    "        return [request_embedding(text_description=text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83770b48-66f8-4ce5-892c-8b4e5248873f",
   "metadata": {},
   "source": [
    "## Instantiate the Custom Class\n",
    "\n",
    "We'll now instantiate the `MultimodalEmbeddings` class to use it in our retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "460b5367-53f9-4e50-8bb2-d26650866524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the custom embedding model\n",
    "embed_model = MultimodalEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c0bb89-87b5-474b-9ee6-de98632ad561",
   "metadata": {},
   "source": [
    "## Create a Simple Retriever\n",
    "\n",
    "We'll create a simple retriever using the custom embedding model and the vector index.  \n",
    "\n",
    "Key configurations:\n",
    "1. **`similarity_top_k`**: Number of top results to retrieve\n",
    "2. **`vector_store_query_mode`**: Set to **\"hybrid\"** for combining semantic and keyword search\n",
    "3. **`alpha`**: Weighting between semantic (embedding) and keyword search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d7d53-5197-4a19-b99b-4efa1295790d",
   "metadata": {},
   "source": [
    "## Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bf9953e-2c52-4443-a80b-eda7a6647334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=vector_index,\n",
    "    embed_model=embed_model,\n",
    "    similarity_top_k=8,  # Retrieve the top 8 results\n",
    "    vector_store_query_mode=\"hybrid\",  # Enable hybrid search\n",
    "    alpha=0.5  # Weighting between semantic and keyword search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a3c96b-14d8-42aa-a1e3-15efcfd2158f",
   "metadata": {},
   "source": [
    "## Query Vector Database Directly Using a Simple Retriever\n",
    "\n",
    "We'll use a simple retriever to query the vector database and inspect the results. This method interacts with the embeddings and metadata in a straightforward way, without utilizing an LLM-powered **Query Engine** or **Chat Engine**.\n",
    "\n",
    "**Why this step matters**:\n",
    "1. Validates that the vector database is populated correctly\n",
    "2. Shows how to query embeddings directly, bypassing the overhead of LLM-based reasoning\n",
    "3. Prepares the groundwork for building advanced workflows with Query Engines and Agents\n",
    "\n",
    "In the next steps, we'll extend this retriever to integrate with an LLM-powered Query Engine for richer responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d11e8-31a9-45ba-9f5c-84af7973df42",
   "metadata": {},
   "source": [
    "## Query the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ff2ff60-aae2-4f5c-a5a2-344323deb67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2.2390\n",
      "Text: Catwalk women red shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.2251\n",
      "Text: Fila men leonard red shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.2235\n",
      "Text: Basics men red casual shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.2148\n",
      "Text: Carlton london women casual red casual shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.2105\n",
      "Text: Nike men jordan fly wade red sports shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.1868\n",
      "Text: Red tape men brown shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.1822\n",
      "Text: Adidas men blue & red f10 sports shoes\n",
      "--------------------------------------------------\n",
      "Score: 2.1682\n",
      "Text: Red tape men casual brown casual shoes\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Query the vector store for \"red shoes\"\n",
    "results = retriever.retrieve(\"red shoes\")\n",
    "\n",
    "# Display results\n",
    "for item in results:\n",
    "    score = item.score\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(f\"Text: {item.get_content()}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5910c5-6a33-41f7-9cdc-5444c1e7cdee",
   "metadata": {},
   "source": [
    "## Visualize Vector Database Pull\n",
    "\n",
    "The vector database query returns a list of red shoes based on the embeddings. To verify the results, let's visualize the pulled vectors.  \n",
    "\n",
    "We'll create a function that loops through the retrieved nodes and displays each image along with its metadata in a row for easy inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d47d7d42-8857-42ac-baed-f08b8a7c7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nodes_with_images_in_row(vector_database_response_nodes, image_folder=\"data/footwear\", img_width=150):\n",
    "    html_content = \"<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\"\n",
    "    \n",
    "    for node in vector_database_response_nodes:\n",
    "        # Retrieve text and product_id from node metadata\n",
    "        text = node.metadata.get('text')\n",
    "        product_id = node.metadata.get('product_id')\n",
    "        \n",
    "        # Generate image path based on product_id\n",
    "        image_path = os.path.join(image_folder, f\"{product_id}.jpg\")\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            # Add each text and image in a flex container\n",
    "            html_content += f\"\"\"\n",
    "                <div style=\"text-align: center;\">\n",
    "                    <p>{text}</p>\n",
    "                    <img src='{image_path}' width='{img_width}px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # Handle missing images gracefully\n",
    "            html_content += f\"\"\"\n",
    "                <div style=\"text-align: center;\">\n",
    "                    <p>{text}</p>\n",
    "                    <p style='color: red;'>Image not found for product_id {product_id}</p>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "\n",
    "    # Close the main div\n",
    "    html_content += \"</div>\"\n",
    "    \n",
    "    # Display the content as HTML\n",
    "    display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dcb914-4064-4164-8c62-0a5506248c22",
   "metadata": {},
   "source": [
    "## Visualize Shoes\n",
    "Let's visualize the shoes retrieved from the vector database to confirm that the results match the query for \"red shoes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "450e5cde-9e95-48d8-a121-5c9462c0837b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Catwalk women red shoes</p>\n",
       "                    <img src='data/footwear/54.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Fila men leonard red shoes</p>\n",
       "                    <img src='data/footwear/48.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Basics men red casual shoes</p>\n",
       "                    <img src='data/footwear/47.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Carlton london women casual red casual shoes</p>\n",
       "                    <img src='data/footwear/53.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men jordan fly wade red sports shoes</p>\n",
       "                    <img src='data/footwear/45.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Red tape men brown shoes</p>\n",
       "                    <img src='data/footwear/71.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Adidas men blue & red f10 sports shoes</p>\n",
       "                    <img src='data/footwear/36.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Red tape men casual brown casual shoes</p>\n",
       "                    <img src='data/footwear/76.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_nodes_with_images_in_row(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98472c-0429-4b65-8d4c-d3d4641b1ab3",
   "metadata": {},
   "source": [
    "## Examine the Shoes\n",
    "\n",
    "As shown, all the retrieved shoes are red or have red details, confirming that the vector index query works well for focused queries.  \n",
    "\n",
    "Next, we'll involve an LLM to add more flexibility to our queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78d0ef-f8f8-4a3d-b03d-6d1e53af856a",
   "metadata": {},
   "source": [
    "## Why Not Keep Using the Index Alone?\n",
    "\n",
    "The results look great, so why involve an LLM?\n",
    "\n",
    "Let's try querying the vector database with something unrelated, like \"Thank you!\" and examine the response.\n",
    "\n",
    "![Why involve an LLM?](images/1_dec/6_naive_rag_reply.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c34129-ca74-4b6b-b0a8-52be521f20b7",
   "metadata": {},
   "source": [
    "## Test Naive Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a17f636-0d3c-4953-b2f2-2f24b8b95203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.1846\n",
      "Text: Timberland women femmes brown boot\n",
      "--------------------------------------------------\n",
      "Score: 1.1784\n",
      "Text: Nike men's air max black shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1670\n",
      "Text: Nike men's egoli white black shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1652\n",
      "Text: Nike women ten blue white shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1650\n",
      "Text: Nike women zoo blue shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1609\n",
      "Text: Adidas women's piona white shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1591\n",
      "Text: Nike women main draw white blue shoe\n",
      "--------------------------------------------------\n",
      "Score: 1.1566\n",
      "Text: Hm women brown shoes\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Timberland women femmes brown boot</p>\n",
       "                    <img src='data/footwear/79.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men's air max black shoe</p>\n",
       "                    <img src='data/footwear/7.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men's egoli white black shoe</p>\n",
       "                    <img src='data/footwear/22.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women ten blue white shoe</p>\n",
       "                    <img src='data/footwear/44.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women zoo blue shoe</p>\n",
       "                    <img src='data/footwear/43.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Adidas women's piona white shoe</p>\n",
       "                    <img src='data/footwear/26.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women main draw white blue shoe</p>\n",
       "                    <img src='data/footwear/28.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Hm women brown shoes</p>\n",
       "                    <img src='data/footwear/81.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the vector store with an unrelated query\n",
    "results = retriever.retrieve(\"Thank you!\")\n",
    "\n",
    "# Display results\n",
    "for item in results:\n",
    "    score = item.score\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(f\"Text: {item.get_content()}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Visualize the results\n",
    "display_nodes_with_images_in_row(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32e025-f4b9-476b-9954-9c577cdd9b19",
   "metadata": {},
   "source": [
    "## Limitations of a Naive RAG System\n",
    "\n",
    "Regardless of the query, the vector database matches the closest vectors based on the embeddings. \n",
    "\n",
    "In this case, querying with \"Thank you!\" still returns shoes because the database doesn't understand the context or intent of the query.\n",
    "\n",
    "This demonstrates the limitation of a **naive RAG (Retrieval-Augmented Generation) system**. \n",
    "\n",
    "While it works well for focused queries like **\"red shoes\"**, it fails to adapt to non-specific or conversational inputs.\n",
    "\n",
    "Here's an illustration of this limitation:\n",
    "\n",
    "![Naive RAG System](images/1_dec/6_naive_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9dc5f-4d99-4e82-bc36-7d4c3854d4ca",
   "metadata": {},
   "source": [
    "## Create a Vector Index Query Engine\n",
    "\n",
    "To overcome the limitations of naive queries, we'll integrate an LLM into our workflow by creating a **Query Engine**.  \n",
    "\n",
    "This Query Engine will:\n",
    "1. Interpret the user's natural language input\n",
    "2. Retrieve contextually relevant information from the vector database\n",
    "3. Enable more dynamic and flexible interactions with the data\n",
    "\n",
    "For this guide, we'll use the `openai-o4` model for the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cd47c-3116-4cfd-94fb-cfb846cd8c62",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c32e8019-e035-45fd-92a8-e4fbf6f96683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = OpenAI_Llama(\n",
    "    temperature=0.0, \n",
    "    model=\"gpt-4o\", \n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba916e82-4e77-4ba3-9c71-c75914193a7d",
   "metadata": {},
   "source": [
    "## Create Query Engine\n",
    "\n",
    "We'll now create a Query Engine using the vector index and our custom embedding model. This engine will leverage the LLM for intelligent query interpretation and responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b040650-ea8f-4b73-9d96-96dce8289377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine from the vector index\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    embed_model=embed_model,\n",
    "    similarity_top_k=8,\n",
    "    vector_store_query_mode=\"hybrid\",\n",
    "    alpha=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231ebc0-593c-457d-9dfb-6ad510618a6c",
   "metadata": {},
   "source": [
    "## Test with Today's Challenge: Woment's Casual Shoes, but Then Suddenly Formal\n",
    "\n",
    "Let's test the Query Engine by asking for women's casual shoes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c38c619b-8230-4f0b-b4d1-fc81f7ede397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:  Here are some options for women's casual shoes:\n",
      "\n",
      "1. Adidas women's piona white shoe - Price: 50\n",
      "2. Carlton London women casual red casual shoes - Price: 90\n",
      "3. Nike women flyclave black casual shoes - Price: 175\n",
      "4. Catwalk women turquoise casual shoes - Price: 65\n"
     ]
    }
   ],
   "source": [
    "# Query the engine for casual women's shoes\n",
    "response = query_engine.query(\"I'm looking for women's casual shoes\")\n",
    "\n",
    "print(\"Chatbot response: \", response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f959705-aa7e-4c28-b93a-e52bf1e69f1e",
   "metadata": {},
   "source": [
    "## Visualize pulled shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65640130-aff4-4557-9511-01178af2a636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Adidas women's piona white shoe</p>\n",
       "                    <img src='data/footwear/26.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Carlton london women casual red casual shoes</p>\n",
       "                    <img src='data/footwear/53.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Lee cooper men's lc casual denim blue shoe</p>\n",
       "                    <img src='data/footwear/33.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men's air max black shoe</p>\n",
       "                    <img src='data/footwear/7.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Adidas men's canon dialect blue yellow shoe</p>\n",
       "                    <img src='data/footwear/31.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men's egoli white black shoe</p>\n",
       "                    <img src='data/footwear/22.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women flyclave black casual shoes</p>\n",
       "                    <img src='data/footwear/12.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Catwalk women turquoise casual shoes</p>\n",
       "                    <img src='data/footwear/40.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_nodes_with_images_in_row(response.source_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e0ed9-55f9-4e98-b0f0-aa4f99794e76",
   "metadata": {},
   "source": [
    "## Examine Results\n",
    "\n",
    "Looking at the results, the chatbot vectorizes the customer query and retrieves products and recommends a variety of women's casual shoes.\n",
    "\n",
    "Here's the chatbot reply:\n",
    "\n",
    "> Here are some options for women's casual shoes:\n",
    ">1. Adidas women's piona white shoe - Price: 50\n",
    ">2. Carlton London women casual red casual shoes - Price: 90\n",
    ">3. Nike women flyclave black casual shoes - Price: 175\n",
    ">4. Catwalk women turquoise casual shoes - Price: 65\n",
    "\n",
    "![The chatbot correctly gave us casual women's shoes](images/2_dec/2_casual_womens_shoes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d2406-0ffd-42e5-ad42-0f40ddee8942",
   "metadata": {},
   "source": [
    "## We suddenly change our minds, we actually need something more formal\n",
    "Wait, I know we said casual shoes, but we actually need something more formal, let's ask the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cba726df-f09e-472b-b7ec-b73d698a14f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:  For a more formal option, consider the Arrow men formal black shoe or the Enroute men leather brown formal shoes.\n"
     ]
    }
   ],
   "source": [
    "# Query the engine for formal alternatives\n",
    "response = query_engine.query(\"Actually, I need something more formal\")\n",
    "\n",
    "print(\"Chatbot response: \", response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f7fb7e-a71d-45b7-8aa6-788bd74d6dad",
   "metadata": {},
   "source": [
    "## Examine results\n",
    "Instead of pulling formal women's shoes, the chatbot now gave us **\"Arrow men formal black shoe\"** or the **\"Enroute men leather brown formal shoes\"**.\n",
    "\n",
    "Why is that?\n",
    "\n",
    "![A naive RAG chatbot processes each user message independently](images/2_dec/4_irrelevant_formal_shoes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe6d9a7-656f-4784-b246-8b86ae59be36",
   "metadata": {},
   "source": [
    "# Visualized pulled shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c581e11-48d2-4389-a1c9-3d7f27524550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Enroute men leather brown formal shoes</p>\n",
       "                    <img src='data/footwear/73.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Arrow men formal black shoe</p>\n",
       "                    <img src='data/footwear/6.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Enroute men leather brown formal shoes</p>\n",
       "                    <img src='data/footwear/74.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men's air max black shoe</p>\n",
       "                    <img src='data/footwear/7.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women main draw white blue shoe</p>\n",
       "                    <img src='data/footwear/28.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Hm women brown shoes</p>\n",
       "                    <img src='data/footwear/81.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike men's egoli white black shoe</p>\n",
       "                    <img src='data/footwear/22.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women zoo blue shoe</p>\n",
       "                    <img src='data/footwear/43.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_nodes_with_images_in_row(response.source_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71577bd4-68b8-4682-8661-f185532493d5",
   "metadata": {},
   "source": [
    "### Why Did the Naive Chatbot Fail?\n",
    "\n",
    "When the customer updates their request, the naive RAG system processes it as a brand-new query without considering the earlier conversation.\n",
    "\n",
    "The naive chatbot doesn't carry over important details from the first message when handling the second one. Each time, it starts fresh:\n",
    "\n",
    "- **No Ongoing Memory:** Doesn't connect **\"Actually, I need something more formal\"** to the earlier **\"women's casual shoes\"** request\n",
    "- **Independent Vectorization:** Treats each message on its own, losing details like \"women's\" or \"shoes\"\n",
    "- **No Context Linking:** Doesn't adjust its search to include both past and present requirements\n",
    "\n",
    "### Limitations Highlighted\n",
    "- **Lack of Conversation Memory:** Forgets earlier information when the user's request changes\n",
    "- **Rigid Query Handling:** Each message is processed as if it's the very first\n",
    "- **Inconsistent Results:** The second answer doesn't build on the first, causing confusion and irrelevant product suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5e103c8-921c-4643-9e47-0c2c36c56280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>category</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>product_type</th>\n",
       "      <th>color</th>\n",
       "      <th>color_details</th>\n",
       "      <th>usage</th>\n",
       "      <th>product_title</th>\n",
       "      <th>image</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>heels_height</th>\n",
       "      <th>brand</th>\n",
       "      <th>titan_embedding</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>86</td>\n",
       "      <td>women</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>heels</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>formal</td>\n",
       "      <td>Carlton london women black heels</td>\n",
       "      <td>86.jpg</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>carlton london</td>\n",
       "      <td>[0.010144724, 0.0051543294, -0.02019924, 0.003...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>87</td>\n",
       "      <td>women</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>heels</td>\n",
       "      <td>nude</td>\n",
       "      <td>[]</td>\n",
       "      <td>formal</td>\n",
       "      <td>Carlton london women nude heels</td>\n",
       "      <td>87.jpg</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>carlton london</td>\n",
       "      <td>[0.01662784, 0.0043598893, -0.03550609, 0.0269...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>women</td>\n",
       "      <td>footwear</td>\n",
       "      <td>shoes</td>\n",
       "      <td>heels</td>\n",
       "      <td>black</td>\n",
       "      <td>[]</td>\n",
       "      <td>formal</td>\n",
       "      <td>Catwalk women corporate leather black heels</td>\n",
       "      <td>88.jpg</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>catwalk</td>\n",
       "      <td>[0.017327517, 0.002226485, -0.01856189, 0.0036...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_id gender  category sub_category product_type  color  \\\n",
       "85          86  women  footwear        shoes        heels  black   \n",
       "86          87  women  footwear        shoes        heels   nude   \n",
       "87          88  women  footwear        shoes        heels  black   \n",
       "\n",
       "   color_details   usage                                product_title   image  \\\n",
       "85            []  formal             Carlton london women black heels  86.jpg   \n",
       "86            []  formal              Carlton london women nude heels  87.jpg   \n",
       "87            []  formal  Catwalk women corporate leather black heels  88.jpg   \n",
       "\n",
       "    price_usd  heels_height           brand  \\\n",
       "85        200             2  carlton london   \n",
       "86        200             2  carlton london   \n",
       "87        155             1         catwalk   \n",
       "\n",
       "                                      titan_embedding token_count  \n",
       "85  [0.010144724, 0.0051543294, -0.02019924, 0.003...           9  \n",
       "86  [0.01662784, 0.0043598893, -0.03550609, 0.0269...           9  \n",
       "87  [0.017327517, 0.002226485, -0.01856189, 0.0036...           9  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the dataset for formal women's shoes\n",
    "df_shoes[(df_shoes['usage'] == 'formal') & (df_shoes['gender'] == 'women')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5b041-bce4-48d4-a48e-5f137830059e",
   "metadata": {},
   "source": [
    "## Query engine limitations\n",
    "\n",
    "This demonstrates another limitation: by not linking the two messages, the chatbot forgets the earlier details and may show results that no longer fit the full picture (women's formal shoes).\n",
    "\n",
    "\n",
    "To address this, we'll now involve an AI Agent which can handle context shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896ff7d5-5def-424e-af29-420b92facf73",
   "metadata": {},
   "source": [
    "## Create Vector Store Info\n",
    "\n",
    "We'll define metadata about the vector store to allow the AI agent to filter results based on specific attributes like gender, usage, and color. This metadata will enhance the agent's ability to refine queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "99b4f201-40ae-4421-a1ac-f47187112fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store information\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"shoes in the shoe store\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"gender\",\n",
    "            type=\"str\",\n",
    "            description=\"Either 'men' or 'women'\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"usage\",\n",
    "            type=\"str\",\n",
    "            description=\"Either 'sports', 'casual', or 'formal'\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"color\",\n",
    "            type=\"str\",\n",
    "            description=(\"Either 'black', 'white', 'blue', 'turquoise blue', 'red', 'pink', 'brown', 'green', or 'multi'\"),\n",
    "        ),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8ada5-b118-4ac0-8e41-40212efb8a81",
   "metadata": {},
   "source": [
    "## Create Tools\n",
    "\n",
    "Tools are essential for enabling the AI Agent to interact with the vector store.  \n",
    "We'll define two tools:\n",
    "1. **`create_metadata_filter`**: Generates metadata filters for refining the search query\n",
    "2. **`search_footwear_database`**: Searches the vector database using the query and optional filters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38a7c5-cec2-4b0f-a569-81473b73af57",
   "metadata": {},
   "source": [
    "## Define Metadata Filter Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "881cd56f-06f4-4b55-a29c-5cd265460194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool to create metadata filters\n",
    "def create_metadata_filter(filter_string):\n",
    "    \"\"\"\n",
    "    Creates metadata filter JSON for vector database queries.\n",
    "\n",
    "    Args:\n",
    "        filter_string (str): Query string for generating metadata filters.\n",
    "\n",
    "    Returns:\n",
    "        str: JSON string of filters.\n",
    "    \"\"\"\n",
    "    class CustomRetriever(VectorIndexAutoRetriever):\n",
    "        def __init__(self, vector_index, vector_store_info, **kwargs):\n",
    "            super().__init__(vector_index, vector_store_info, **kwargs)\n",
    "\n",
    "        def _retrieve(self, query, **kwargs):\n",
    "            query_bundle = QueryBundle(query_str=query)\n",
    "            retrieval_spec = self.generate_retrieval_spec(query_bundle)\n",
    "            return retrieval_spec\n",
    "\n",
    "    llm_filter = OpenAI_Llama(\n",
    "        temperature=0.5, # higher temperature than 0\n",
    "        model=\"gpt-4o\",\n",
    "        api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "        system_prompt=\"You are a helpful assistant, help the user purchase shoes.\",\n",
    "    )\n",
    "\n",
    "    custom_retriever = CustomRetriever(vector_index=vector_index, vector_store_info=vector_store_info, llm=llm_filter)\n",
    "    retrieval_spec = custom_retriever._retrieve(filter_string)\n",
    "\n",
    "    filters_dicts = [{'key': f.key, 'value': f.value, 'operator': f.operator.value} for f in retrieval_spec.filters]\n",
    "    return json.dumps(filters_dicts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20794f-1a28-46ca-9030-b809ed93222b",
   "metadata": {},
   "source": [
    "## Define Footwear Vector Database Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f7621ef2-1efe-485f-bfee-d8ff9aaa03da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tool to search the footwear vector database\n",
    "def search_footwear_database(query_str, filters_json=None):\n",
    "    \"\"\"\n",
    "    Searches the footwear vector database using a query string and optional filters.\n",
    "\n",
    "    Args:\n",
    "        query_str (str): Query string describing the footwear.\n",
    "        filters_json (Optional[List]): JSON list of metadata filters.\n",
    "\n",
    "    Returns:\n",
    "        list: Search results from the vector database.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the embedding for the query string\n",
    "    query_embedding = embed_model._get_query_embedding(query_str)\n",
    "\n",
    "    # Deserialize from JSON\n",
    "    metadata_filters = MetadataFilters.from_dicts(filters_json, condition=FilterCondition.AND)\n",
    "    \n",
    "    vector_store_query = VectorStoreQuery(\n",
    "        query_str=query_str,\n",
    "        query_embedding=query_embedding,\n",
    "        alpha=0.5,\n",
    "        mode='hybrid',\n",
    "        filters=metadata_filters,\n",
    "        similarity_top_k=8\n",
    "    )\n",
    "    \n",
    "    # Execute the query against the vector store\n",
    "    query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "    # Create output without embeddings\n",
    "    nodes_with_scores = []\n",
    "    for index, node in enumerate(query_result.nodes):\n",
    "        score: Optional[float] = None\n",
    "        if query_result.similarities is not None:\n",
    "            score = query_result.similarities[index]\n",
    "        nodes_with_scores.append({\n",
    "            'color': node.metadata['color'],\n",
    "            'text': node.metadata['text'],\n",
    "            'gender': node.metadata['gender'],\n",
    "            'product_type': node.metadata['product_type'],\n",
    "            'product_id': node.metadata['product_id'],\n",
    "            'brand': node.metadata['brand'],\n",
    "            'usage': node.metadata['usage'],\n",
    "            'price': node.metadata['price'],\n",
    "            'similarity_score': score\n",
    "        })\n",
    "\n",
    "    return nodes_with_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0366ee68-aa49-48c9-b97e-580bdb56a686",
   "metadata": {},
   "source": [
    "## Define Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a1950185-56bb-4567-ae17-d989ff09da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_metadata_filters_tool = FunctionTool.from_defaults(\n",
    "    name=\"create_metadata_filter\",\n",
    "    fn=create_metadata_filter\n",
    ")\n",
    "\n",
    "query_vector_database_tool = FunctionTool.from_defaults(\n",
    "    name=\"search_footwear_database\",\n",
    "    fn=search_footwear_database\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c703c-3841-4ab4-badc-49cfb4300344",
   "metadata": {},
   "source": [
    "## Create AI Agent\n",
    "\n",
    "We'll now define an AI Agent capable of reasoning over the data, generating filters, and performing refined searches to address customer queries more effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77219b3-92c2-468d-a6ba-04dac8d88dfd",
   "metadata": {},
   "source": [
    "### Create the agent worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f69b032d-a57b-4c5e-b890-649e00d36d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent worker\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [\n",
    "        create_metadata_filters_tool,\n",
    "        query_vector_database_tool,\n",
    "    ],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    system_prompt=\"\"\"\\\n",
    "You are an agent designed to answer customers looking for shoes.\\\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "Drive sales and always feel free to ask a user for more information.\\\n",
    "\n",
    "- Always consider if filters are needed based on the user's query.\n",
    "- Use the tools provided to answer questions; do not rely on prior knowledge.\n",
    "- Always feel free to ask a user for more information.\n",
    "\n",
    "**Example 1:**\n",
    "\n",
    "User Query: \"Hi! I'm going to a party and I'm looking for red women's shoes. Thank you!\"\n",
    "\n",
    "Agent Actions:\n",
    "\n",
    "1. Determine what query string to use for filters e.g. \"red woman's shoes\"\n",
    "2. Call:\n",
    "   filter_string = create_metadata_filter_string(\"red woman's shoes\")\n",
    "3. Call:\n",
    "   results = search_footwear_database(query_str='shoes', filter_string=filter_string)\n",
    "\n",
    "**Example 2:**\n",
    "\n",
    "User Query: \"Hi! I'm going to a meeting and I'm looking for formal women's shoes. Thank you!\"\n",
    "\n",
    "Agent Actions:\n",
    "\n",
    "1. Determine what query string to use for filters e.g. \"formal woman's shoes\"\n",
    "2. Call:\n",
    "   filter_string = create_metadata_filter_string(\"formal woman's shoes\")\n",
    "3. Call:\n",
    "   results = search_footwear_database(query_str='shoes', filter_string=filter_string)\n",
    "\n",
    "**Example 3:**\n",
    "\n",
    "User Query: \"I'm looking for shoes\"\n",
    "\n",
    "Agent Actions:\n",
    "\n",
    "1. Ask for more information\n",
    "\n",
    "Remember to follow these instructions carefully.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450ec80-0572-4d3c-991f-d2c2efc13e9f",
   "metadata": {},
   "source": [
    "### Create the agent runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "db72d671-b970-422a-a6c5-1231f79d8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12346a04-5a1a-4606-91a1-44bd5907808b",
   "metadata": {},
   "source": [
    "## Test Agent\n",
    "\n",
    "Let's test the AI Agent by asking for casual women's shoes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3719234e-c0ae-48a0-8e1a-d2297e2ee8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: I'm looking for women's casual shoes\n",
      "=== Calling Function ===\n",
      "Calling function: create_metadata_filter with args: {\"filter_string\": \"women's casual shoes\"}\n",
      "=== Function Output ===\n",
      "[{\"key\": \"gender\", \"value\": \"women\", \"operator\": \"==\"}, {\"key\": \"usage\", \"value\": \"casual\", \"operator\": \"==\"}]\n",
      "=== Calling Function ===\n",
      "Calling function: search_footwear_database with args: {\"query_str\": \"shoes\", \"filters_json\": [{\"key\": \"gender\", \"value\": \"women\", \"operator\": \"==\"}, {\"key\": \"usage\", \"value\": \"casual\", \"operator\": \"==\"}]}\n",
      "=== Function Output ===\n",
      "[{'color': 'brown', 'text': 'Hm women brown shoes', 'gender': 'women', 'product_type': 'flats', 'product_id': 81, 'brand': 'hm', 'usage': 'casual', 'price': 155, 'similarity_score': 1.73761463}, {'color': 'brown', 'text': 'Gliders women brown shoes', 'gender': 'women', 'product_type': 'casual shoes', 'product_id': 80, 'brand': 'gliders', 'usage': 'casual', 'price': 75, 'similarity_score': 1.71296632}, {'color': 'red', 'text': 'Catwalk women red shoes', 'gender': 'women', 'product_type': 'casual shoes', 'product_id': 54, 'brand': 'catwalk', 'usage': 'casual', 'price': 75, 'similarity_score': 1.7024461}, {'color': 'multi', 'text': 'Rocia women multi- coloured shoes', 'gender': 'women', 'product_type': 'flats', 'product_id': 69, 'brand': 'rocia', 'usage': 'casual', 'price': 130, 'similarity_score': 1.70161736}, {'color': 'black', 'text': 'Skechers women black casual shoes', 'gender': 'women', 'product_type': 'casual shoes', 'product_id': 11, 'brand': 'skechers', 'usage': 'casual', 'price': 185, 'similarity_score': 1.70154941}, {'color': 'black', 'text': 'Reebok women black casual shoes', 'gender': 'women', 'product_type': 'flats', 'product_id': 13, 'brand': 'reebok', 'usage': 'casual', 'price': 85, 'similarity_score': 1.69552517}, {'color': 'black', 'text': 'Nike women flyclave black casual shoes', 'gender': 'women', 'product_type': 'casual shoes', 'product_id': 12, 'brand': 'nike', 'usage': 'casual', 'price': 175, 'similarity_score': 1.68524623}, {'color': 'turquoise blue', 'text': 'Catwalk women turquoise casual shoes', 'gender': 'women', 'product_type': 'casual shoes', 'product_id': 40, 'brand': 'catwalk', 'usage': 'casual', 'price': 65, 'similarity_score': 1.6850152}]\n",
      "=== LLM Response ===\n",
      "Here are some women's casual shoes you might like:\n",
      "\n",
      "1. **Hm Women Brown Shoes**\n",
      "   - Type: Flats\n",
      "   - Brand: Hm\n",
      "   - Price: $155\n",
      "\n",
      "2. **Gliders Women Brown Shoes**\n",
      "   - Type: Casual Shoes\n",
      "   - Brand: Gliders\n",
      "   - Price: $75\n",
      "\n",
      "3. **Catwalk Women Red Shoes**\n",
      "   - Type: Casual Shoes\n",
      "   - Brand: Catwalk\n",
      "   - Price: $75\n",
      "\n",
      "4. **Rocia Women Multi-Coloured Shoes**\n",
      "   - Type: Flats\n",
      "   - Brand: Rocia\n",
      "   - Price: $130\n",
      "\n",
      "5. **Skechers Women Black Casual Shoes**\n",
      "   - Type: Casual Shoes\n",
      "   - Brand: Skechers\n",
      "   - Price: $185\n",
      "\n",
      "6. **Reebok Women Black Casual Shoes**\n",
      "   - Type: Flats\n",
      "   - Brand: Reebok\n",
      "   - Price: $85\n",
      "\n",
      "7. **Nike Women Flyclave Black Casual Shoes**\n",
      "   - Type: Casual Shoes\n",
      "   - Brand: Nike\n",
      "   - Price: $175\n",
      "\n",
      "8. **Catwalk Women Turquoise Casual Shoes**\n",
      "   - Type: Casual Shoes\n",
      "   - Brand: Catwalk\n",
      "   - Price: $65\n",
      "\n",
      "Let me know if you need more information or if there's anything else I can help with!\n"
     ]
    }
   ],
   "source": [
    "# Test the agent\n",
    "agent_response = agent.chat(\"I'm looking for women's casual shoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0583c-0fd2-4e60-9b48-d5eae29ec1af",
   "metadata": {},
   "source": [
    "### Interpreting the Query\n",
    "\n",
    "Unlike a naive query engine, which vectorizes the full user query, the AI agent approaches the request by changing the query and add a custom filter.\n",
    "\n",
    "For the query **\"I'm looking for women's casual shoes\"**, the agent decides to not vectorize the entire customer query. Instead, the agent vectorizes **shoes** and then creates a filter for the condition **women's casual shoes** and then recommends 8 different shoes\n",
    "\n",
    "![Agent ensures it fully understands the customer's needs](images/2_dec/6_ai_agent_options.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d27b7-9bec-4e9d-964d-86b7e4a6f99e",
   "metadata": {},
   "source": [
    "# Visualize the Agent's Recommendations\n",
    "Let's create a function that visualizes the agent's recommended shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5c7f083d-7a9a-4558-9483-8da61fc702e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_agent_response(agent_response, image_folder=\"data/footwear\", img_width=150, threshold=98):\n",
    "    \"\"\"\n",
    "    Visualizes products from agent response if they match (fuzzily) names in an unstructured string.\n",
    "\n",
    "    Args:\n",
    "    - agent_response: Agent response.\n",
    "    - image_folder: Path to the folder containing product images.\n",
    "    - img_width: Width of the product images in the visualization.\n",
    "    - threshold: Minimum similarity score for fuzzy matching.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays the visualization directly in the notebook.\n",
    "    \"\"\"\n",
    "    # Extract product names from raw output and make them lowercase\n",
    "    products = [product['text'].lower() for product in agent_response.sources[1].raw_output]\n",
    "\n",
    "    # Prepare HTML content for visualization\n",
    "    html_content = \"<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\"\n",
    "\n",
    "    # Loop through the products and match with unstructured string\n",
    "    for product in agent_response.sources[1].raw_output:\n",
    "        product_name = product['text'].lower()\n",
    "\n",
    "        # Perform fuzzy matching\n",
    "        match = process.extractOne(product_name, [agent_response.response.lower()], scorer=fuzz.partial_ratio)\n",
    "        if match and match[1] > threshold:  # If a match is found and meets the threshold\n",
    "            # Generate image path based on product_id\n",
    "            image_path = os.path.join(image_folder, f\"{product['product_id']}.jpg\")\n",
    "\n",
    "            # Append product info and image to HTML content\n",
    "            html_content += f\"\"\"\n",
    "                <div style=\"text-align: center;\">\n",
    "                    <p>{product['text']}</p>\n",
    "                    <img src='{image_path}' width='{img_width}px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "\n",
    "    # Close the main div\n",
    "    html_content += \"</div>\"\n",
    "\n",
    "    # Display the content as HTML\n",
    "    display(HTML(html_content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2917656e-5507-4671-ba4d-4bfe560c210f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Hm women brown shoes</p>\n",
       "                    <img src='data/footwear/81.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Gliders women brown shoes</p>\n",
       "                    <img src='data/footwear/80.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Catwalk women red shoes</p>\n",
       "                    <img src='data/footwear/54.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Skechers women black casual shoes</p>\n",
       "                    <img src='data/footwear/11.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Reebok women black casual shoes</p>\n",
       "                    <img src='data/footwear/13.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Nike women flyclave black casual shoes</p>\n",
       "                    <img src='data/footwear/12.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Catwalk women turquoise casual shoes</p>\n",
       "                    <img src='data/footwear/40.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function\n",
    "visualize_agent_response(agent_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a70b8e-a5ad-4398-ab5e-e4e22226cce2",
   "metadata": {},
   "source": [
    "## We suddenly change our minds, we actually need something more formal\n",
    "Let's change our minds again and ask for something more formal instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "24b2e762-59b2-4904-b3d6-b63b6b220c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Actually, I need something more formal\n",
      "=== Calling Function ===\n",
      "Calling function: create_metadata_filter with args: {\"filter_string\": \"women's formal shoes\"}\n",
      "=== Function Output ===\n",
      "[{\"key\": \"gender\", \"value\": \"women\", \"operator\": \"==\"}, {\"key\": \"usage\", \"value\": \"formal\", \"operator\": \"==\"}]\n",
      "=== Calling Function ===\n",
      "Calling function: search_footwear_database with args: {\"query_str\": \"shoes\", \"filters_json\": [{\"key\": \"gender\", \"value\": \"women\", \"operator\": \"==\"}, {\"key\": \"usage\", \"value\": \"formal\", \"operator\": \"==\"}]}\n",
      "=== Function Output ===\n",
      "[{'color': 'black', 'text': 'Catwalk women corporate leather black heels', 'gender': 'women', 'product_type': 'heels', 'product_id': 88, 'brand': 'catwalk', 'usage': 'formal', 'price': 155, 'similarity_score': 1.19167948}, {'color': 'black', 'text': 'Carlton london women black heels', 'gender': 'women', 'product_type': 'heels', 'product_id': 86, 'brand': 'carlton london', 'usage': 'formal', 'price': 200, 'similarity_score': 1.18591034}, {'color': 'nude', 'text': 'Carlton london women nude heels', 'gender': 'women', 'product_type': 'heels', 'product_id': 87, 'brand': 'carlton london', 'usage': 'formal', 'price': 200, 'similarity_score': 1.18491364}]\n",
      "=== LLM Response ===\n",
      "Here are some options for women's formal shoes:\n",
      "\n",
      "1. **Catwalk Women Corporate Leather Black Heels**\n",
      "   - Type: Heels\n",
      "   - Price: $155\n",
      "   - Brand: Catwalk\n",
      "\n",
      "2. **Carlton London Women Black Heels**\n",
      "   - Type: Heels\n",
      "   - Price: $200\n",
      "   - Brand: Carlton London\n",
      "\n",
      "3. **Carlton London Women Nude Heels**\n",
      "   - Type: Heels\n",
      "   - Price: $200\n",
      "   - Brand: Carlton London\n",
      "\n",
      "If you have any specific preferences or need further assistance, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "# Specify preferences in the query\n",
    "agent_response = agent.chat(\"Actually, I need something more formal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7b70e-474a-44cc-910f-4e6bc9306a2e",
   "metadata": {},
   "source": [
    "# Visualize the agents recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "210ed1fc-fea3-439c-b4e2-2f5d63fdbfd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='display: flex; flex-wrap: wrap; gap: 20px;'>\n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Catwalk women corporate leather black heels</p>\n",
       "                    <img src='data/footwear/88.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Carlton london women black heels</p>\n",
       "                    <img src='data/footwear/86.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            \n",
       "                <div style=\"text-align: center;\">\n",
       "                    <p>Carlton london women nude heels</p>\n",
       "                    <img src='data/footwear/87.jpg' width='150px' style=\"border: 1px solid #ddd; padding: 5px;\"/>\n",
       "                </div>\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the function\n",
    "visualize_agent_response(agent_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e2bd0-9285-4858-99e8-158299981437",
   "metadata": {},
   "source": [
    "# Detailed Agent Workflow\n",
    "## Agent Workflow\n",
    "\n",
    "When refining the query from \"women's casual shoes\" to \"women's formal shoes,\" the agent takes the following steps:\n",
    "\n",
    "1. **Context Tracking:**\n",
    "    - Retains the original focus on \"women's shoes\" from the earlier part of the conversation\n",
    "    - Recognizes that the context has shifted from \"casual\" to \"formal\" without losing track of gender  \n",
    "\n",
    "\n",
    "2. **Tool Invocation:**\n",
    "    - Calls the `create_metadata_filter` function with the argument `filter_string=\"women's formal shoes\"`, generating the following metadata filter:\n",
    "\n",
    "    ```json\n",
    "    [\n",
    "      {\"key\": \"gender\", \"value\": \"women\", \"operator\": \"==\"}, \n",
    "      {\"key\": \"usage\", \"value\": \"formal\", \"operator\": \"==\"}\n",
    "    ]\n",
    "    ```\n",
    "    - Calls the `search_footwear_database` function with the query string `\"shoes\"` and the generated filter. This refines the search to include only women's formal shoes.\n",
    "\n",
    "3. **Response Generation:**\n",
    "    - Combines the retrieved vector database results with its retained memory to provide a response that acknowledges the context shift:\n",
    "\n",
    ">Here are some options for women's formal shoes:\n",
    ">\n",
    ">1. **Catwalk Women Corporate Leather Black Heels**\n",
    ">2. **Carlton London Women Black Heels**\n",
    ">3. **Carlton London Women Nude Heels**\n",
    ">If you have any specific preferences or need further assistance, feel free to let me know!\n",
    "\n",
    "![Agent workflow](images/2_dec/5_ai_agent_memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c4671-7cb1-4eb1-b56d-9f665e6d9c07",
   "metadata": {},
   "source": [
    "## Why the AI Agent Succeeds\n",
    "\n",
    "The AI agent keeps track of the whole conversation. \n",
    "\n",
    "When the customer changes their mind, the AI agent doesn't forget the original focus on women's shoes. \n",
    "\n",
    "Instead, it updates the search from **\"casual\"** to **\"formal\"** while still looking for women's footwear.\n",
    "\n",
    "1. **Conversation Memory:** Remembers the earlier detail - women's shoes - and updates only the **\"casual\"** part to **\"formal\"**\n",
    "2. **Flexible Reasoning:** Adapts the vectorized query without starting from zero each time\n",
    "3. **Accurate Results:** Finds women's formal shoes that match the new requirement\n",
    "\n",
    "## Key Takeaways\n",
    "### Naive Chatbot Limitation\n",
    "- Doesn't remember earlier requests and treats new messages as unrelated searches\n",
    "\n",
    "### AI Agent Advantages\n",
    "- **Conversation Memory:** Maintains a running understanding of the conversation\n",
    "- **Flexible Reasoning:** Updates the search based on the latest input without losing previous details\n",
    "- **Accurate Results:** Delivers results that stay relevant as the user's needs evolve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9a0f0-2b1e-4c25-85c6-bedbf2b2bd49",
   "metadata": {},
   "source": [
    "## Conclusion: Day 2 - How AI agents improve naive chatbots by understanding context shifts\n",
    "\n",
    "This example shows how a naive RAG chatbot fails when the user changes their mind mid-conversation. By not connecting the dots, it provides unhelpful results. \n",
    "\n",
    "An AI agent, on the other hand, can smoothly adapt to the changing request, keeping track of what was said before and making sure the recommendations stay on target.\n",
    "\n",
    "Stay tuned for tomorrow's issue, where we'll explore another challenge and see how AI agents handle it better than simple chatbots.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Tomorrow, we'll continue our journey by tackling another challenge faced by naive chatbots.\n",
    "\n",
    "---\n",
    "\n",
    "### Ready to implement this in your own systems?  \n",
    "AI agents are the future of e-commerce chatbots. If you're interested in applying these concepts to your business, feel free to reach out!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881144e-893c-48e8-90ad-ae311972e7b5",
   "metadata": {},
   "source": [
    "# Like this repo? \n",
    "Iâ€™m working on a course to help you build and deploy your own AI agent chatbot from scratch. [Sign up here!](https://braine.ai/#agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf82f75-230c-4197-9e9e-a3e006dcd59a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca4933-57c8-43e2-96a5-287f079abcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
